{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-09_dimensionality_reduction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"imgDjRpBGlke","colab_type":"text"},"source":["#4-9 次元圧縮"]},{"cell_type":"markdown","metadata":{"id":"a4jheVRPh8I8","colab_type":"text"},"source":["## 概要\n","**次元圧縮は、データの次元を圧縮することによって、データの構造を見やすくしたり、機械学習の計算量を軽減して計算スピードを向上させたりする手法です。**\n","\n","代表的な手法に、主成分分析（Primary Component Analysis, PCA）、t-SNE法があります。"]},{"cell_type":"markdown","metadata":{"id":"32tZKLDGMLSn","colab_type":"text"},"source":["##主成分分析（PCA）"]},{"cell_type":"markdown","metadata":{"id":"T3UGC9Txn6KB","colab_type":"text"},"source":["主成分分析は、相関のある複数の変数を、ばらつきの方向と大きさに着目し、より相関の少ない合成関数に変換して、データの次元を縮約する手法です。\n","\n","下図に2 次元データの例を示します。このデータのばらつき（＝情報）は右肩上がりの方向が最も大きく見えます。\n","\n","<img src=\"https://github.com/t-date/DataScience/blob/master/fig/04_09_03.jpg?raw=true\" width=\"280px\">\n","\n","そこで、上図のように、この右肩上がりの方向の軸を第一主成分軸と定義し、直交する軸を第二主成分軸と定義します。この新しい軸にしたがって回転した図が下図になります。\n","\n","<img src=\"https://github.com/t-date/DataScience/blob/master/fig/04_09_04.jpg?raw=true\" width=\"350px\">\n","\n","この新たな軸は、このデータの情報をよく表していることになります。このように、データのばらつきの方向と大きさに着目した分析法が主成分分析です。次元圧縮をする際は、よりばらつきの小さい第二主成分軸方向の情報を削減し、第一主成分軸方向に圧縮します。\n","\n","<img src=\"https://github.com/t-date/DataScience/blob/master/fig/04_09_05.jpg?raw=true\" width=\"350px\">\n","\n","こうすることにより、より多くデータの情報を残しつつ次元を削減することができます。このとき、主成分の寄与度を表すものを主成分得点（主成分スコア）、観測変数との相関を表すものを主成分負荷量もしくは因子負荷量といい、主成分分析の結果の評価に用いられます。\n"]},{"cell_type":"markdown","metadata":{"id":"SaPeY_1WLxAt","colab_type":"text"},"source":["###主成分分析の実装(あやめの例)"]},{"cell_type":"markdown","metadata":{"id":"pskORvXUOeqd","colab_type":"text"},"source":["教師なし学習の例として、アイリスデータの次元を減らしてより簡単に可視化する方法を見てみましょう。\n","\n","アイリスのデータは4 次元でり、各サンプルは4 つの特徴を持ちます。\n","データの本質的な特徴を保持したまま適切な低次元表現にできるかが次元削減の課題です。多くの場合、次元の削減はデータを可視化するのに役立ちます。4 次元以上のデータをプロットするより2次元のデータをプロットする方がはるかに簡単です。\n","\n","ここでは、主成分分析を使用します。これは、高速な線形の次元削減手法です。モデルに対して2 つの要素、つまりデータの2 次元表現を返すよう要求します。\n"]},{"cell_type":"markdown","metadata":{"id":"6lio8hWdSBsJ","colab_type":"text"},"source":["Seabornライブラリを使用して、あやめのデータセットをpandas DataFrame の形式でダウンロードします。"]},{"cell_type":"code","metadata":{"id":"d-9jYXgTR71Z","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","iris = sns.load_dataset('iris')\n","iris.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fAWtml1SURD","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import seaborn as sns; sns.set()\n","sns.pairplot(iris, hue='species', height=1.5);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xgDyqhrSh-n","colab_type":"text"},"source":["scikit-learnのために、DataFrameから特徴行列と目的配列を抽出します。"]},{"cell_type":"code","metadata":{"id":"rhqLSYKVSpYN","colab_type":"code","colab":{}},"source":["X_iris = iris.drop('species', axis=1)\n","X_iris.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdwYosb_SrUL","colab_type":"code","colab":{}},"source":["y_iris = iris['species']\n","y_iris.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0IDD6Q3RdXa","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA # 1.モデルクラスを選択する\n","\n","model = PCA(n_components=2) # 2. ハイパーパラメータを添えて、インスタンス化する\n","model.fit(X_iris) # 3.データへの当てはめを行う（yを指定していない点に注意）\n","X_2D = model.transform(X_iris) # 4. 2 次元のデータに変換する"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbIbVRD9S_f0","colab_type":"text"},"source":["結果をプロットします。元のirisデータフレームに結果を追加して、Seabornのlmplot を使用すします。"]},{"cell_type":"code","metadata":{"id":"2FWyv8K0Pn_8","colab_type":"code","colab":{}},"source":["iris['PCA1'] = X_2D[:, 0]\n","iris['PCA2'] = X_2D[:, 1]\n","sns.lmplot(\"PCA1\", \"PCA2\", hue='species', data=iris, fit_reg=False);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5YiV4gdTE8n","colab_type":"text"},"source":["PCAアルゴリズムに種のラベルを与えていないにもかかわらず、種がかなりよく分離されていることが2次元表現上でわかります。\n","このデータセットに対しては、比較的簡単な分類が有効であることを示しています。"]},{"cell_type":"markdown","metadata":{"id":"55rXYEX-RE5j","colab_type":"text"},"source":["###主成分分析の実装(手書き文字の例)"]},{"cell_type":"markdown","metadata":{"id":"sNDnwRmDMDPA","colab_type":"text"},"source":["scikit-learnに入っている手書き数字データセット（digitsデータセット）に主成分分析を適用してみましょう。\n","\n","このデータセットの個々のデータポイントは8×8のグレースケールの0から9までの手書き数字です。"]},{"cell_type":"code","metadata":{"id":"i00fhCmkMPkp","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set() # プロットのスタイルを決める\n","import numpy as np\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n","                         subplot_kw={'xticks':(), 'yticks': ()})\n","for ax, img in zip(axes.ravel(), digits.images):\n","  ax.imshow(img)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNAVuYetMdZk","colab_type":"text"},"source":["PCAを使ってこのデータを2次元にして可視化してみましょう。最初の2つの主成分を用い、各点をクラスごとに色分けしています。"]},{"cell_type":"code","metadata":{"id":"-Eh87-IAMhj0","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","# PCAモデルを構築\n","pca = PCA(n_components=2)\n","pca.fit(digits.data)\n","# 数値データを最初の2主成分で変形\n","digits_pca = pca.transform(digits.data)\n","colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n","          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n","plt.figure(figsize=(10, 10))\n","plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n","plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n","for i in range(len(digits.data)):\n","  # 散布図を数字でプロット\n","  plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n","           color = colors[digits.target[i]],\n","           fontdict={'weight': 'bold', 'size': 9})\n","plt.xlabel(\"First principal component\")\n","plt.ylabel(\"Second principal component\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sigezBmgNJRs","colab_type":"text"},"source":["ここでは、どのクラスがどこにあるかを示すために各点をその数字で表現している。数字0と6と4は最初の2主成分で比較的うまく分離できているが、それでも重なっている。他の数字は大きく重なり合っている。"]},{"cell_type":"markdown","metadata":{"id":"mRNVPQYLmTgY","colab_type":"text"},"source":["##t-SNE法\n"]},{"cell_type":"markdown","metadata":{"id":"rfELBD4BLXZO","colab_type":"text"},"source":["**t-SNE 法は、高次元のデータを、自由度１のt 分布を用いて２次元や３次元の低次元に圧縮する手法です。**\n","\n","t-SNEの各文字は、それぞれ下記を表します。\n","\n","*   t:t分布\n","*   S:確率的（Stochastic）\n","*   N:隣接（Neighbor）\n","*   E:埋め込み（Embedding）\n","\n","この手法で次元圧縮すると、離れたグループはより離れて配置されるため、クラスタリングしやすくなるという特徴があります。一方で、圧縮後の次元が4 次元以上だとうまく働かない場合もあるため、2次元もしくは3 次元が推奨されています。\n","下図に主成分分析とt-SNE法による次元圧縮結果の比較例を示します。\n","\n","<img src=\"https://github.com/t-date/DataScience/blob/master/fig/04_09_06.jpg?raw=true\" width=\"700px\">\n"]},{"cell_type":"markdown","metadata":{"id":"9CFQnvilNSGi","colab_type":"text"},"source":["###t-SNE法の実装(手書き文字の例)"]},{"cell_type":"markdown","metadata":{"id":"0nw8mTKaNWVm","colab_type":"text"},"source":["主成分分析で実装した手書き数字データセット（digitsデータセット）に対し、結果を比較するために、t-SNEを適用してみましょう。\n","\n","t-SNEは新しいデータの変換をサポートしていないため、TSNEクラスにはtransformメソッドがありません。これに代えて、fit_transformメソッドを利用します。このメソッドはモデルを作ると同時に、それを使って変換してデータを返します。"]},{"cell_type":"code","metadata":{"id":"l5ESLai_Nx5J","colab_type":"code","colab":{}},"source":["from sklearn.manifold import TSNE\n","tsne = TSNE(random_state=42)\n","# fitではなくfit_transformを用いる。TSNEにはtransformメソッドがない\n","digits_tsne = tsne.fit_transform(digits.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KQzVnt0N7FH","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10, 10))\n","plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n","plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n","for i in range(len(digits.data)):\n","  # 点ではなく数字をテキストとしてプロットする\n","  plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n","           color = colors[digits.target[i]],\n","           fontdict={'weight': 'bold', 'size': 9})\n","plt.xlabel(\"t-SNE feature 0\")\n","plt.xlabel(\"t-SNE feature 1\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5H9PLuKuNkXa","colab_type":"text"},"source":["t-SNEの結果では、すべてのクラスがかなり明確に分離されています。1と9は少し別れているが、ほとんどのクラスは1つの密な集団にまとまっています。\n","\n","この方法では、クラスラベルの知識をまったく使っていません。これは完全に教師なし学習なのだ。にも関わらず、データをクラスごとにきれいに分離して2次元に表現する方法を、もとの空間の点の近さだけを使って発見したわけであります。\n","\n","t-SNEアルゴリズムにはチューニングパラメータがいくつかあるが、デフォルトの設定で大抵はうまく機能します。perplexityやearly_exaggerationをいじってみてもよいが、一般に効果は大きくはありません。"]}]}