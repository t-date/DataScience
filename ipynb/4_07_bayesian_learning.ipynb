{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"4_07_bayesian_learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XQhZsl486mlO","colab_type":"text"},"source":["#4-7 ベイジアン学習"]},{"cell_type":"markdown","metadata":{"id":"94o6nyCAaTeC","colab_type":"text"},"source":["## 概要"]},{"cell_type":"markdown","metadata":{"id":"rVeR2ov16mlS","colab_type":"text"},"source":["**ベイジアン学習は、条件付き確率を使用した機械学習アルゴリズムで、ベイズの定理を利用して結果から原因を推論することを特徴としています。**\n","\n","スパムメールフィルタや\n","ＥＣサイトのレコメンデーションなど、実社会の様々な場所に活用されています。\n","\n","*   ベイズの定理\n","> ある条件A のもとで、事象Ｂが起こる確率（条件付き確率または事後確率）をP(B¦A)と表すと、事象B が起こったときに条件Ａであった事前確率P(A¦B)は、次のように表せます。これをベイズの定理といいます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_07_01.jpg?raw=true\" width=\"200px\">\n","\n","上の式は「原因の確率」を推算する式であり、ある結果を導く原因を推論するた\n","めに様々な用途で用いられています。\n","\n","*   尤度\n","> あるデータが与えられたとき、どのような確率分布が最もよくデータの分布を表\n","すかをベイズの定理を用いて推定するとします。そのとき、確率分布の尤（もっと）\n","もらしさを表す指標を尤度といいます。\n","尤度は、尤度関数を用いて計算されます。\n","尤度関数は、条件付き確率と紐づいた関数のため、負の値はとらず、積分すると１\n","になります。ただし、確率密度関数とは別の概念です。尤度関数を最大化するパラメー\n","タを推定する手法を最尤法（Method of maximum likelihood）といいます。最尤法は1922 年にロナルド・フィッシャーが論文で初めて使用した用語です。\n","\n","\n","*   ナイーブベイズ分類器（ベイジアンフィルタ）\n","> ナイーブベイズモデルは、別名単純ベイズモデルと呼ばれ、事象同士が独立であると仮定した条件付き確率のモデルです。ナイーブベイズ分類器は、シンプルで処理が高速なので、文書分類やメールのスパムフィルタなどに広く用いられています。\n","一方で、単純なモデルで単語間の意味関係は処理できないため、精度が高くないとも指摘されています。\n","\n","*   ベイジアンネットワーク\n","> ベイジアンネットワークは、原因と結果の複数の組み合わせを有向グラフで可視化した確率モデル（グラフィカルモデル）の一つです。1985年にジュディア・パールによって命名されました。パールは、この功績によりチューリング賞を受賞しています。\n","\n","下図に単純なベイジアンネットワークの例を示します。ベイジアンネットワークは、各ノードがマルコフ性を満たす、つまり各ノードの状態が条件付き独立であることで、計算を大幅に簡略化できます。\n","ベイジアンネットワークは様々な原因と結果を推測することが可能であり、主観的な情報も対象にできるため、適用範囲が広いアルゴリズムです。クリストファー・\n","ビショップによる書籍などによって認知が広まったことと、コンピュータの計算能力向上により多数のノードを持つベイジアンネットワークの確率推論が可能になり、近年研究と活用が活発に進んでいます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_07_02.jpg?raw=true\" width=\"650px\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vHh3GSEE86Zx","colab_type":"text"},"source":["##ナイーブベイズクラス分類器の実装\n","ナイーブベイズモデルは非常に高速で簡単な分類アルゴリズムの1 つであり、非常に高次元の\n","データセットに適しています。このアルゴリズムは非常に高速で、チューニング可能なパラメータ\n","がとても少ないため、分類問題の最初の一手として非常に役立ちます。\n","\n","ナイーブベイズ分類器がどのように動作するかを直感的に説明し、いくつかのデータセットに対する例を示します。"]},{"cell_type":"markdown","metadata":{"id":"VPRH8A4K8-kd","colab_type":"text"},"source":["###ガウシアンナイーブベイズ\n","ナイーブベイズ分類器（Naive Bayes classifiers）はベイズ分類法に基づいて作られています。これは統計量に対する条件付き確率の関係を説明するベイズの定理に依存しています。\n","\n","ベイズ分類で\n","は、ある特徴量（features）が観測された際に、そのラベルがL である確率に興味があります。これをP(L | features)と記述します。ベイズの定理は、これを直接的に計算できる量から求める方法を与えます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_07_03.jpg?raw=true\" width=\"360px\">\n","\n","2 つのラベル（ここではL1とL2としましょう）を決定することを考えます。1 つの方法は各ラベルの事後確率の比を計算することです。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_07_04.jpg?raw=true\" width=\"360px\">\n","\n","各ラベルに対するP(features | Li)を計算するためのモデルが必要です。このようなモデルは、データを生成する仮想的なランダムプロセスを指定するため、生成モデル（generative model）と呼ばれます。\n","各ラベルに対してこの生成モデルを指定することが、ベイズ分類器学習の主要な部分となります。\n","\n","一般化された学習を行うのは非常に難しい作業なのですが、いくつかのモデルに対する単純な前提を取り入れることで簡単に行うことができます。\n","これが「ナイーブベイズ」の「ナイーブ」（単純）の理由です。各ラベルの生成モデルについて非常に単純な仮定をすると、各クラスに対する生成モデルのおおよその近似を使って、ベイズ分類が行えます。さまざまなタイプのナイーブベイズ分類器は、データに関するそれぞれの単純な仮定に基づいています。以下の節でいくつかの例を見てみましょう。まず標準的なモジュールをインポートします。"]},{"cell_type":"code","metadata":{"id":"eJwZuLmM_bmu","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCZN02CO_zHK","colab_type":"text"},"source":["ナイーブベイズ分類器の中でおそらく最も単純なものは、ガウシアンナイーブベイズ分類器でしょう。この分類器では、各ラベルからのデータが単純なガウス分布に従うという仮定に基づいています。次のデータで考えてみましょう。"]},{"cell_type":"code","metadata":{"id":"Wvrp1EqY8-OT","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YYd4MaKo_9Oo","colab_type":"text"},"source":["データが次元間の共分散を伴わないガウス分布に基づいていると仮定することで、非常に簡単に\n","モデルが作成できます。モデルへの当てはめを行うには、各ラベルに対するデータポイントの平均\n","と標準偏差を見つけるだけで可能となります。分布を定義するのに値は、その2 つだけだからです。\n","この単純なガウス推定の結果を下図に示します。"]},{"cell_type":"code","metadata":{"id":"HtvtPzJKAFZ8","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import GaussianNB\n","model = GaussianNB()\n","model.fit(X, y)\n","#model.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PhWNcLq7CZbp","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n","\n","fig, ax = plt.subplots()\n","\n","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n","ax.set_title('Naive Bayes Model', size=14)\n","\n","xlim = (-8, 8)\n","ylim = (-15, 5)\n","\n","xg = np.linspace(xlim[0], xlim[1], 60)\n","yg = np.linspace(ylim[0], ylim[1], 40)\n","xx, yy = np.meshgrid(xg, yg)\n","Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n","\n","for label, color in enumerate(['red', 'blue']):\n","    mask = (y == label)\n","    mu, std = X[mask].mean(0), X[mask].std(0)\n","    P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n","    Pm = np.ma.masked_array(P, P < 0.03)\n","    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5,\n","                  cmap=color.title() + 's')\n","    ax.contour(xx, yy, P.reshape(xx.shape),\n","               levels=[0.01, 0.1, 0.5, 0.9],\n","               colors=color, alpha=0.2)\n","    \n","ax.set(xlim=xlim, ylim=ylim)\n","\n","#fig.savefig('figures/05.05-gaussian-NB.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjAwsVOdFYiC","colab_type":"text"},"source":["この楕円は、各ラベルに対するガウシアン生成モデルを表し、楕円の中心に向かうほど高い確率を示します。この生成モデルをクラスごとに配置することで、任意のデータポイントに対する尤度P(features | L1)を計算する簡単な手法が得られます。そのため、事後確率比を簡単に計算し、各データポイントのラベルが何であるのが最も確からしいかを判定できます。\n","\n","この手順はscikit-learnのsklearn.naive_bayes.GaussianNB 推定器で実装されています。"]},{"cell_type":"markdown","metadata":{"id":"o4RUOXzMFjpJ","colab_type":"text"},"source":["新しいデータを生成し、ラベルを予測してみましょう。"]},{"cell_type":"code","metadata":{"id":"c4-ld1xvFP_3","colab_type":"code","colab":{}},"source":["rng = np.random.RandomState(0)\n","Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n","ynew = model.predict(Xnew)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oIPuKG9kFo3g","colab_type":"text"},"source":["この新しいデータをプロットして、境界がどこにあるのかを可視化できます"]},{"cell_type":"code","metadata":{"id":"IckW-jHcFp2r","colab_type":"code","colab":{}},"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n","lim = plt.axis()\n","plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n","plt.axis(lim);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rMo3tDGFwUt","colab_type":"text"},"source":["境界はわずかに湾曲しています。一般に、ガウシアンナイーブベイズの境界は2 次曲線となります。\n","\n","ベイズ主義の良いところは、確率的な分類が可能である点です。これはpredict_proba メソッドを使って計算できます。"]},{"cell_type":"code","metadata":{"id":"sfeyFbdaFzEn","colab_type":"code","colab":{}},"source":["yprob = model.predict_proba(Xnew)\n","yprob[-8:].round(2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tXp0AajBF2dC","colab_type":"text"},"source":["結果の列は、それぞれ第1 および第2 ラベルに対する事後確率を表します。分類の不確実性についての推定値を求めたいのであれば、このようなベイズのアプローチは非常に有用です。\n","\n","もちろん、分類の最終的な良し悪しは、モデルに対して行った仮定の良し悪しに依存します。ガウシアンナイーブベイズの結果がそれほど良いものとならない理由がここにあります。それでも、多くの場合、特に特徴の数が多くなるにつれて、こうした単純な仮定がガウシアンナイーブベイズの有用性を高めています。"]},{"cell_type":"markdown","metadata":{"id":"PmNnLxdpGCCx","colab_type":"text"},"source":["###多項分布ナイーブベイズ\n","前の例で説明したガウスの仮定は、各ラベルの生成分布を指定する際に使用できる単純な仮定の1つにすぎません。もう1 つの有用な例は、単純な多項分布から生成されると仮定した多項分布ナ\n","イーブベイズ（Multinomial Naive Bayes）です。多項分布は、複数のカテゴリがどれだけ観測されるかの確率を表しているため、多項分布ナイーブベイズは出現数または出現レートレートを表す特徴量に最も適しています。\n","\n","最良のガウス分布を持つデータ分布をモデル化するのではなく、最適な多項分布を用いてデータ分布をモデル化するという点を除けば、考え方は前の例とまったく同じです。"]},{"cell_type":"markdown","metadata":{"id":"Xz8FftMXGLfs","colab_type":"text"},"source":["多項分布ナイーブベイズが頻繁に使用される問題の1 つが、文書内の語数または単語の出現頻度に関連するテキスト分類です。\n","\n","ここでは、20個のニュースグループのコーパスから得られる単語数を用い、短い文書をカテゴリに分類する方法を示します。\n","データをダウンロードして、ニュースグループ名を見てみましょう。"]},{"cell_type":"code","metadata":{"id":"dkn_eP5_GYwM","colab_type":"code","colab":{}},"source":["from sklearn.datasets import fetch_20newsgroups\n","data = fetch_20newsgroups()\n","data.target_names"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jwRcEc56Gd_Z","colab_type":"text"},"source":["簡略化のために、対象のカテゴリを絞り、学習セットとテストセットをダウンロードします。"]},{"cell_type":"code","metadata":{"id":"hTaQlv7vGfQn","colab_type":"code","colab":{}},"source":["categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space',\n","'comp.graphics']\n","train = fetch_20newsgroups(subset='train', categories=categories)\n","test = fetch_20newsgroups(subset='test', categories=categories)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpbykwJNGkil","colab_type":"text"},"source":["代表的なデータを1 つ見てみましょう。"]},{"cell_type":"code","metadata":{"id":"ulzHRp3OGl8g","colab_type":"code","colab":{}},"source":["print(train.data[5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74XYkYiMGoia","colab_type":"text"},"source":["このデータを機械学習で使用するには、各データの内容を数値のベクトルに変換する必要があります。このために、TF-IDFベクトル化器を使用し、多項分布ナイーブベイズ分類器に接続するパイプラインを作成します。"]},{"cell_type":"code","metadata":{"id":"H9M3Kk05Gse9","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","model = make_pipeline(TfidfVectorizer(), MultinomialNB())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RGHeOIzGub9","colab_type":"text"},"source":["このパイプラインを使用して、モデルを学習データに適用し、テストデータのラベルを予測できます。"]},{"cell_type":"code","metadata":{"id":"fJWXQnVuGvT6","colab_type":"code","colab":{}},"source":["model.fit(train.data, train.target)\n","labels = model.predict(test.data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wajhlZ-NGzd6","colab_type":"text"},"source":["テストデータのラベルを予測したので、推定値の評価が行えます。例えば、テストデータの真の\n","ラベルと予測したラベルの間の混同行列を次に示します。"]},{"cell_type":"code","metadata":{"id":"pMNXfyu7G5vX","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(test.target, labels)\n","sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","xticklabels=train.target_names, yticklabels=train.target_names, cmap='RdPu')\n","plt.xlabel('true label')\n","plt.ylabel('predicted label')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Pyx6QurHOXE","colab_type":"text"},"source":["確かに、この非常に単純な分類器であっても、宇宙に関する話題とコンピュータに関する話題を\n","うまく分類できましたが、宗教に関する話題とキリスト教に関する話題では混乱しました。おそら\n","くこれは想定内の混乱です。\n","\n","素晴らしいことに、このパイプラインのpredict() メソッドを使用すれば、任意の文字列のカテゴリを決定するツールになります。ここでは、一連の文字列に対する分類予測を返す簡単なユーティリティ関数を示します。"]},{"cell_type":"code","metadata":{"id":"zdADsDvOHUIC","colab_type":"code","colab":{}},"source":["def predict_category(s, train=train, model=model):\n","  pred = model.predict([s])\n","  return train.target_names[pred[0]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3iIeQOmYHWDM","colab_type":"text"},"source":["いくつか試してみましょう。"]},{"cell_type":"code","metadata":{"id":"um0NfLn6HKzy","colab_type":"code","colab":{}},"source":["predict_category('sending a payload to the ISS')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKWqsdrXH3XJ","colab_type":"code","colab":{}},"source":["predict_category('discussing islam vs atheism')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fM0ZjDW5H4ar","colab_type":"code","colab":{}},"source":["predict_category('determining the screen resolution')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1rta0SDH683","colab_type":"text"},"source":["これは、文字列内の各単語の（重み付けされた）頻度による単純な確率モデルと比べて、それほど洗練されたものではないことを覚えておいてください。\n","\n","それにもかかわらず、得られる結果は印象的です。非常に単純なアルゴリズムであっても、慎重に使用し、高次元の大きなデータセットで\n","学習したならその結果は驚くほど効果的となります。"]},{"cell_type":"markdown","metadata":{"id":"SzMo6xu7ID1N","colab_type":"text"},"source":["ナイーブベイズ分類器はデータ対して単純な仮定をするため、複雑なモデルと同じようには機能しません。とは言うものの、いくつかの利点があります。\n","*   学習と予測のどちらも非常に高速です\n","*   わかりやすい確率的予測を提供します\n","*   多くの場合で、非常に解釈が簡単です\n","*   調整可能なパラメータが非常に少数です\n","\n","れらの利点は、最初に使用する分類器として、ナイーブベイズ分類器が良い選択であることを意味します。それがうまく働くのであれば、非常に高速で非常にわかりやすい分類器が得られたこ\n","とになります。うまくいかない場合は、より洗練されたモデルの調査を始めましょう。どうなれば良いのか、その基準とすべき知識は、ナイーブベイズ分類器の結果から既に得られています。\n","\n","ナイーブベイズ分類器は、次のような状況に対して特に優れています。\n","*   単純な仮定がデータと一致する場合（実際には非常にまれです）\n","*   モデルの複雑性がそれほど重要ではなく、カテゴリが非常によく分離されている場合\n","*   モデルの複雑性がそれほど重要ではなく、データの次元が非常に高い場合\n","\n","最後の2つは異なるように見えますが、実際には関連しています。データセットの次元が拡大するにつれて、2つの点が近くにある可能性は低くなります（結局、2 つの点がすべての次元で近くになければ、総合的に2 つの点が近傍にあることにはなりません）。\n","\n","これは、新しい次元には情報が追加されていると仮定するなら、高次元のクラスタは低次元のクラスタよりも平均的に離散する傾向があることを意味します。この理由から、ナイーブベイズのように単純な分類器は、次元が増えるにつれてより複雑な分類器よりもうまく機能する傾向があります。データが十分であれば、単純なモデルでも強力になります。\n","\n","\n","\n","\n","\n","\n","\n"]}]}