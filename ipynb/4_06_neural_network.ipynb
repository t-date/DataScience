{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"4_06_neural_network.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XQhZsl486mlO","colab_type":"text"},"source":["#4-6 ニューラルネットワーク"]},{"cell_type":"markdown","metadata":{"id":"94o6nyCAaTeC","colab_type":"text"},"source":["## 概要"]},{"cell_type":"markdown","metadata":{"id":"rVeR2ov16mlS","colab_type":"text"},"source":["**ニューラルネットワークは、動物の神経システムを模倣した学習モデルの総称です。**\n","\n","ニューラルネットワークは、回帰問題、分類問題の両方に適用できますが、分類問題によく使われます。\n","\n","\n","ニューラルネットワークは、以下のような複数の層を持つ構造になっています。\n","入力層は、入力されたデータそのものを持ちます。\n","出力層は、分類問題の場合には、その分類ラベル数分の出力があり、出力時は各ラベルの確率を出力します。\n","隠れ層は中間層とも呼ばれ、この隠れ層を積み重ねることで、複雑な決定領域を学習できます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_06_06.jpg?raw=true\" width=\"500px\">\n","\n","意外なことに、人工ニューラルネットワーク（artificial neural networks：ANN)の考え方はかなり古くからあります。神経生理学者のウォーレン・マカロックと数学者のウォルター・ピッツが1943 年に初めてこの考え方を提出しました。\n","\n","1960 年代までの初期のANN の成功により、多くの人々が本当の意味で知的なマシンと会話できる日が近いと考えるようになりました。しかし、この約束が満たされないだろう（少なくともかなり\n","長い間）ということが明らかになると、資金は別の分野に投入されるようになり、ANN は長い暗黒時代に入りました。\n","\n","1980 年代始めには、新しいネットワークアーキテクチャの発明と従来よりも優れた訓練テクニックの開発により、ANN に対する関心が蘇ったが、1990 年代までは、ほとんどの研究者たちはサポートベクトルマシンなどのほかの機械学習テクニックを高く評価していました。それらの方がよい結果を生み出し、理論的な基礎が強力に見えたのであります。\n","\n","そして最近になり、ANN に対する新たな関心の高まりを目撃することになりました。この波は、今までの波と同じ\n","ように消えてしまうのでしょうか。今度こそANN は私たちの生活に従来よりもはるかに深い影響を与えています。そう考えてよい理由がいくつかあるります。\n","\n","\n","\n","*   今はニューラルネットワークを訓練するための膨大なデータがある。そして、極端に大規模で複雑な問題では、ANN はほかのML テクニックよりも高い性能を示すことが頻繁にある。\n","*   1990 年代以降の計算能力の非常に大幅な強化により、今では大規模なニューラルネットワークを合理的な時間内に訓練できるようになりました。これはムーアの法則のおかげであり、\n","数百万単位で強力なGPU カードを生み出してきたゲーム産業のおかげでもある。\n","*   訓練アルゴリズムが改良されてきている。公平に言って、1990 年代のアルゴリズムとの違\n","いはごくわずかだが、この比較的小さな改良が莫大なプラスの効果を生み出している。\n","*   ANN の理論的な限界のいくつかが実践上無害だということが明らかになった。たとえば、\n","ANN の訓練アルゴリズムは局所的な最適値に捕まってしまうため、失敗が運命づけられて\n","いると多くの人々が考えてきたが、実際にはそうなるのはまれだということが明らかになっ\n","た（実際にそうなった場合でも、通常その局所的最適値は全体の最適値にかなり近い）。\n","*   ANN は、資金を獲得して進歩するというよい循環に入ったように見える。ANN を基礎と\n","する優れた製品がコンスタントに新聞の見出しを飾り、それによってANN に対する関心と\n","資金がさらに集まり、結果として進歩が早まり、さらに素晴らしい製品が生み出されている。\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MYPdHMwCpbp6","colab_type":"text"},"source":["環境設定"]},{"cell_type":"code","metadata":{"id":"ntjtvzc4pZtA","colab_type":"code","colab":{}},"source":["# Python 2, 3 をサポートします\n","from __future__ import division, print_function, unicode_literals\n","\n","# 標準ライブラリのインポート\n","import numpy as np\n","import os\n","\n","# 乱数の固定\n","np.random.seed(42)\n","\n","# プロットの設定\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# 保存先ディレクトリの設定\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"ann\"\n","IMAGE_DIR = \"images\"\n","\n","os.makedirs(os.path.join(PROJECT_ROOT_DIR, IMAGE_DIR, CHAPTER_ID), exist_ok=True)\n","\n","def image_path(fig_id):\n","    return os.path.join(PROJECT_ROOT_DIR, IMAGE_DIR, CHAPTER_ID, fig_id)\n","\n","def save_fig(fig_id, tight_layout=True):\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGE6GNeDmgOE","colab_type":"text"},"source":["##パーセプトロン"]},{"cell_type":"markdown","metadata":{"id":"funDwo8hp3NZ","colab_type":"text"},"source":["パーセプトロン（perceptron）は、ANN アーキテクチャでももっとも単純なもののひとつで、1957 年にフランク・ローゼンブラットによって考え出されました。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_06_08.jpg?raw=true\" width=\"420px\">\n","\n","scikit-learn は、単一のLTU ネットワークを実装するPerceptron クラスを提供しています。こ\n","のクラスは、みなさんの予想通りの動きをする。たとえば、iris データセットで訓練するときには、次のように書きます。"]},{"cell_type":"code","metadata":{"id":"7dY0VZ_QptyV","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import Perceptron\n","\n","iris = load_iris()\n","X = iris.data[:, (2, 3)]  # 花弁の長さ、花弁の幅\n","y = (iris.target == 0).astype(np.int)\n","\n","per_clf = Perceptron(max_iter=100, tol=-np.infty, random_state=42)\n","per_clf.fit(X, y)\n","\n","y_pred = per_clf.predict([[2, 0.5]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1kMY23Qp-T4","colab_type":"code","colab":{}},"source":["y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIj-5wKzqLr1","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n","b = -per_clf.intercept_ / per_clf.coef_[0][1]\n","\n","axes = [0, 5, 0, 2]\n","\n","x0, x1 = np.meshgrid(\n","        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n","        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n","    )\n","X_new = np.c_[x0.ravel(), x1.ravel()]\n","y_predict = per_clf.predict(X_new)\n","zz = y_predict.reshape(x0.shape)\n","\n","plt.figure(figsize=(10, 4))\n","plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n","plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n","\n","plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n","from matplotlib.colors import ListedColormap\n","custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n","\n","plt.contourf(x0, x1, zz, cmap=custom_cmap)\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"lower right\", fontsize=14)\n","plt.axis(axes)\n","\n","save_fig(\"perceptron_iris_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXfOCEc5qi2V","colab_type":"text"},"source":["パーセプトロンの学習アルゴリズムが確率的勾配降下法と非常によく似ていることに気づかれたかもしれません。実際、scikit-learn のPerceptron クラスは、ハイパーパラメータをloss=\"perceptron\"、learning_rate=\"constant\"、eta0=1（学習率）、penalty=None\n","（正則化なし）に設定したSGDClassifier を使うのと同じであります。\n","\n","ロジスティック回帰分類器とは異なり、パーセプトロンはクラスに属する確率を出力しないことに注意が必要です。パーセプトロンは、ハードなしきい値に基づいて分類をするだけであります。これは、パーセプトロンよりもロジスティック回帰を使うべきよい理由のひとつになって\n","います。"]},{"cell_type":"markdown","metadata":{"id":"eBbDz7-6rmpk","colab_type":"text"},"source":["パーセプトロンの重大な弱点として、ごく簡\n","単な問題（たとえば、排他的OR: Exclusive OR、XOR 分類問題。下図左）を解決できないこともそのなかに含まれています。もちろん、これはほかの線形分類モデル（ロジスティック回帰分類器など）にも当てはまることではあります。\n","\n","しかし、パーセプトロンの限界の一部は、複数のパーセプトロンを積み上げることによって取り除けることがわかりました。そのようなANN をMLP（multi-layer perceptron： 多層パーセプトロン）と呼ぶ。特に、MLP は、個々の入力の組み合わせに対して下図の右側に描かれている\n","MLP の出力を計算すれば確かめられるように、XOR 問題を解決できます。ネットワークは、入力が(0, 0) か(1, 1) なら0、入力が(0, 1) か(1, 0) なら1 を出力します。\n","\n","パーセプトロンの限界の一部は、複数のパーセプトロンを積み上げることによって取り除けることがわかっています。そのようなANN をMLP（multi-layer perceptron： 多層パーセプトロン）と呼びます。特に、MLP は、個々の入力の組み合わせに対して図10-6 の右側に描かれている\n","MLP の出力を計算すれば確かめられるように、XOR 問題を解決できる。ネットワークは、入力\n","が(0, 0) か(1, 1) なら0、入力が(0, 1) か(1, 0) なら1 を出力します。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_06_07.jpg?raw=true\" width=\"400px\">\n"]},{"cell_type":"markdown","metadata":{"id":"E2tpnP0ntfsV","colab_type":"text"},"source":["##MLPとバックプロパゲーション(誤差逆伝播法)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X63PuSvAt-Yp","colab_type":"text"},"source":["MLP は、ひとつの（パススルー）入力層と隠れ層（hidden layer）と呼ばれるひとつ以上のLTU層、出力層（output layer）と呼ばれる最後のひとつのLTU 層から構成されます。出力層を除く各層にはバイアスニューロンが含まれており、次の層と完全に接続されています。ANN\n","が複数の隠れ層を保つ場合、そのANN は深層ニューラルネットワーク（deep neural network、DNN）と呼ばれます。\n","\n","MLP を訓練する方法として、バックプロパゲーション（backpropagation：誤差逆伝播法）という訓練アルゴリズムを導入するようになりました。\n","\n","バックプロパゲーションは、個々の訓練インスタンスをネットワークに与え、連続する層のすべてのニューロンの出力を計算します。\n","\n","バックプロパゲーションは、ロジスティック関数以外の活性化関数（activation function）のもとでも使えます。ロジ\n","スティック関数以外でよく使われるふたつの活性化関数を紹介します。\n","\n","*   双曲線正接（hyperbolic tangent）関数、tanh(z) = 2σ(2z) − 1\n","> ロジスティック関数と同様に、S 字形で連続で微分可能だが、出力は−1 から1 までの範囲になる（ロジスティック関数のように0 から1 までではなく）ので、訓練を始めたばかりのときの各層の出力が多少なりとも正規化される（つまり0 に中心に集まる）。これは収束までの時間を短縮するために役立つことが多い。\n","\n","\n","\n","*   ReLU 関数\n","\n","\n","> ReLU(z) = max(0, z)、連続だがz = 0 で微分可能ではない（傾斜が急激に変わる場所。\n","これの存在により勾配降下法が跳ね回る場合がある）。しかし、実際に使ってみると非常に\n","よく機能し、短時間で計算できるというメリットがある。なによりも重要なのは、出力の最大値がないことが勾配降下法の問題緩和に役立つことである。\n","\n"]},{"cell_type":"code","metadata":{"id":"N7xHtLJSwwRU","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def derivative(f, z, eps=0.000001):\n","    return (f(z + eps) - f(z - eps))/(2 * eps)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mSKH2zrw1xV","colab_type":"code","colab":{}},"source":["z = np.linspace(-5, 5, 200)\n","\n","plt.figure(figsize=(11,4))\n","\n","plt.subplot(121)\n","plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n","plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n","plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n","plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n","plt.grid(True)\n","plt.legend(loc=\"center right\", fontsize=14)\n","plt.title(\"Activation functions\", fontsize=14)\n","plt.axis([-5, 5, -1.2, 1.2])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WDJib7Ww-Gt","colab_type":"code","colab":{}},"source":["def heaviside(z):\n","    return (z >= 0).astype(z.dtype)\n","\n","def mlp_xor(x1, x2, activation=heaviside):\n","    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zroVZIJxAUu","colab_type":"code","colab":{}},"source":["x1s = np.linspace(-0.2, 1.2, 100)\n","x2s = np.linspace(-0.2, 1.2, 100)\n","x1, x2 = np.meshgrid(x1s, x2s)\n","\n","z1 = mlp_xor(x1, x2, activation=heaviside)\n","z2 = mlp_xor(x1, x2, activation=sigmoid)\n","\n","plt.figure(figsize=(10,4))\n","\n","plt.subplot(121)\n","plt.contourf(x1, x2, z1)\n","plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n","plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n","plt.title(\"Activation function: heaviside\", fontsize=14)\n","plt.grid(True)\n","\n","plt.subplot(122)\n","plt.contourf(x1, x2, z2)\n","plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n","plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n","plt.title(\"Activation function: sigmoid\", fontsize=14)\n","plt.grid(True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmvfB6Z5v12M","colab_type":"text"},"source":["MLP は、個々の出力がバイナリクラス（たとえば、スパムとハム、緊急と非緊急など）のいずれかになるので、分類でよく使われます。クラスが相互排他的な場合（たとえば、数字イメージの分類における0 から9 までのクラス）には、出力層は、個別の活性化関数ではなく、共有のソフトマックス（softmax）関数を使うように変更されます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_06_10.jpg?raw=true\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"Ogh0fnVFwVox","colab_type":"text"},"source":["##TensorFlowの高水準API を使ったMLPの訓練"]},{"cell_type":"markdown","metadata":{"id":"MWd_khMdwYZJ","colab_type":"text"},"source":["TensorFlow でMLP を訓練する方法として単純なのは、scikit-learn 互換のAPI を提供するTF Learn という高水準API を使うことです。DNNClassifier クラスを使えば、隠れ層がいくつあってもDNN の訓練は非常に簡単であり、ソフトマックス出力層はあるクラスに属する\n","推計確率を出力します。たとえば、次のコードは、ふたつの隠れ層（ひとつはニューロンが300 個、\n","もうひとつはニューロンが100 個）とニューロンが10 個のソフトマックス出力層を持つDNN を\n","分離のために訓練します。"]},{"cell_type":"code","metadata":{"id":"k5TTYPhE03vI","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IKmNZOCB22Ok","colab_type":"text"},"source":["MNISTデータ(0～9の手書き文字データ)を取り込みます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_06_11.jpg?raw=true\" width=\"420px\">"]},{"cell_type":"code","metadata":{"id":"vfBQ1LTo0934","colab_type":"code","colab":{}},"source":["(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n","X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n","y_train = y_train.astype(np.int32)\n","y_test = y_test.astype(np.int32)\n","X_valid, X_train = X_train[:5000], X_train[5000:]\n","y_valid, y_train = y_train[:5000], y_train[5000:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"369_N_s01Cfj","colab_type":"code","colab":{}},"source":["feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[28 * 28])]\n","dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10,\n","                                     feature_columns=feature_cols)\n","\n","input_fn = tf.estimator.inputs.numpy_input_fn(\n","    x={\"X\": X_train}, y=y_train, num_epochs=10, batch_size=50, shuffle=True)#num_epochs=40, batch_size=50, shuffle=True)\n","dnn_clf.train(input_fn=input_fn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kc652cSC1hYI","colab_type":"text"},"source":["このコードは、まず訓練セット（カテゴリ値の列なども含まれている）から数値の列を集めたものを作っています。\n","\n","次に、DNNClassifier を作り、scikit-learn 互換ヘルパーでラップし、最後に指定回数のインスタンスのバッチを使って訓練イテレーションを指定試行回数実行しています。\n","MNIST データセットを対象としてこのコードを実行すると（たとえば、scikit-learn のStandardScaler などを使ってスケーリングしたあとに）、テストセットに対して高い正解率を達成するモデルが得られます。"]},{"cell_type":"code","metadata":{"id":"o4w97a5a2I2Q","colab_type":"code","colab":{}},"source":["test_input_fn = tf.estimator.inputs.numpy_input_fn(\n","    x={\"X\": X_test}, y=y_test, shuffle=False)\n","eval_results = dnn_clf.evaluate(input_fn=test_input_fn)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4lLoAu02LZ_","colab_type":"code","colab":{}},"source":["eval_results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUlBQNJg2NMG","colab_type":"code","colab":{}},"source":["y_pred_iter = dnn_clf.predict(input_fn=test_input_fn)\n","y_pred = list(y_pred_iter)\n","y_pred[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMu-ZW-T3I5p","colab_type":"text"},"source":["##(参考)MLPClassifierの例1"]},{"cell_type":"markdown","metadata":{"id":"KMhc0QHz3nkY","colab_type":"text"},"source":["手書き文字認識での例を試してみましょう"]},{"cell_type":"code","metadata":{"id":"qN4sORat4SwJ","colab_type":"code","colab":{}},"source":["from sklearn.datasets import load_digits\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# データ読み込み\n","data = load_digits()\n","X = data.images.reshape(len(data.images), -1)\n","y = data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) #ホールドアウト\n","model = model = MLPClassifier(hidden_layer_sizes=(16, ))  #隠れ層の数\n","model.fit(X_train, y_train) # 学習\n","y_pred = model.predict(X_test) \n","accuracy_score(y_pred, y_test) # 評価"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-xE1MkyH4QYV","colab_type":"text"},"source":["##(参考)MLPClassifierの例2"]},{"cell_type":"code","metadata":{"id":"zh1bQLhB3eDI","colab_type":"code","colab":{}},"source":["#スクラッチの描画ライブラリをインポート\n","!pip install mglearn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wI52cRT3mBw","colab_type":"text"},"source":["MLPClassifierを、これまでも使ってきたtwo_moonsデータセットに適用して、MLPが動く様\n","子を見てみよう"]},{"cell_type":"code","metadata":{"id":"8fPNx8ga3ihD","colab_type":"code","colab":{}},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import mglearn\n","\n","from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n","random_state=42)\n","mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n","mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n","mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":0,"outputs":[]}]}