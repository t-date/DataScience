{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"4_03_SVM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XQhZsl486mlO","colab_type":"text"},"source":["#4-3 サポートベクトルマシン"]},{"cell_type":"markdown","metadata":{"id":"CaIwG4ix8dh_","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"94o6nyCAaTeC","colab_type":"text"},"source":["## 概要"]},{"cell_type":"markdown","metadata":{"id":"rVeR2ov16mlS","colab_type":"text"},"source":["**サポートベクトルマシンは、データ集合を分類するための手法です。**\n","\n","サポートベクトルマシンでは、データを分類するための境界線とデータの最短距離をマージンとし、このマージンを最大化することで、値を分類するのに良い決定境界線を求めます。\n","マージンとは、学習データのうち最も決定境界線に近いものと、決定境界線との距離です。　※下記はマージン最大化のイメージ\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_03_01.jpg?raw=true\" width=\"320px\">\n","\n","マージンの内側にデータが入ることを許容しないことをハードマージンと呼びます。\n","\n","一部のデータがマージンの内側に入ることを許容することをソフトマージンと呼びます。ソフトマージンで、一部の誤分類を寛容にするためにスラック変数と呼ばれる変数を用います。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_03_02.jpg?raw=true\" width=\"480px\">\n","\n","サポートベクトルマシンには、決定境界が線形の線形サポートベクトルマシン、\n","決定境界が非線型のカーネル法のサポートベクトルマシンがあります。カーネル法\n","で、高速に計算するために、計算量を大幅に削減する方法をカーネルトリックと呼びます。\n","\n","サポートベクトルマシン（SVM）は、線形／非線形分類、回帰だけでなく、外れ値検出さえできる非常に強力で柔軟な機械学習モデルであります。機械学習でもっとも人気のあるモデルのひとつでありSVM は複雑ながら中小規模のデータセットの分類に特に適しています。"]},{"cell_type":"markdown","metadata":{"id":"1F97QvjMEUM6","colab_type":"text"},"source":["## アヤメデータを用いた実装"]},{"cell_type":"markdown","metadata":{"id":"n1Wk0-vjGc8R","colab_type":"text"},"source":["環境設定を行います"]},{"cell_type":"code","metadata":{"id":"Nl-vN0qlGfBb","colab_type":"code","colab":{}},"source":["# Python 2, 3 をサポートします\n","from __future__ import division, print_function, unicode_literals\n","\n","# 標準ライブラリのインポート\n","import numpy as np\n","import os\n","\n","# 乱数の固定\n","np.random.seed(42)\n","\n","# プロットの設定\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# 保存先ディレクトリの設定\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"svm\"\n","IMAGE_DIR = \"images\"\n","\n","os.makedirs(os.path.join(PROJECT_ROOT_DIR, IMAGE_DIR, CHAPTER_ID), exist_ok=True)\n","\n","def image_path(fig_id):\n","    return os.path.join(PROJECT_ROOT_DIR, IMAGE_DIR , CHAPTER_ID, fig_id)\n","\n","def save_fig(fig_id, tight_layout=True):\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18Vja9EDP7_P","colab_type":"text"},"source":["iris (あやめ) データセットを使ってSVMをじっそうしてみます。\n","\n","iris（あやめ）は、セトサ（Iris-Setosa）、バージカラー（Iris-Versicolor）、バージニカ（Iris-Virginica）の3 種類のあやめのがく片（sepal）と花弁（petal）の幅と長さが収められたデータセットです。(前段で説明)\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_06_03.jpg?raw=true\" width=\"460px\">"]},{"cell_type":"markdown","metadata":{"id":"5Eh3pUHlSp7s","colab_type":"text"},"source":["###マージンの大きい分類"]},{"cell_type":"code","metadata":{"id":"awyI8GeQSoY1","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n","y = iris[\"target\"]\n","\n","setosa_or_versicolor = (y == 0) | (y == 1)\n","X = X[setosa_or_versicolor]\n","y = y[setosa_or_versicolor]\n","\n","# SVM Classifier model\n","svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n","svm_clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzSs6CPVHZ2v","colab_type":"code","colab":{}},"source":["x0 = np.linspace(0, 5.5, 200)\n","pred_1 = 5*x0 - 20\n","pred_2 = x0 - 1.8\n","pred_3 = 0.1 * x0 + 0.5\n","\n","def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n","    w = svm_clf.coef_[0]\n","    b = svm_clf.intercept_[0]\n","\n","    #  w0*x0 + w1*x1 + b = 0の決定境界で\n","    # => x1 = -w0/w1 * x0 - b/w1\n","    x0 = np.linspace(xmin, xmax, 200)\n","    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n","\n","    margin = 1/w[1]\n","    gutter_up = decision_boundary + margin\n","    gutter_down = decision_boundary - margin\n","\n","    svs = svm_clf.support_vectors_\n","    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n","    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n","    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n","    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n","\n","plt.figure(figsize=(12,2.7))\n","\n","plt.subplot(121)\n","plt.plot(x0, pred_1, \"g--\", linewidth=2)\n","plt.plot(x0, pred_2, \"m-\", linewidth=2)\n","plt.plot(x0, pred_3, \"r-\", linewidth=2)\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"upper left\", fontsize=14)\n","plt.axis([0, 5.5, 0, 2])\n","\n","plt.subplot(122)\n","plot_svc_decision_boundary(svm_clf, 0, 5.5)\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.axis([0, 5.5, 0, 2])\n","\n","save_fig(\"large_margin_classification_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oRYTgaPIH7J1","colab_type":"text"},"source":["上の図はiris データセットの一部を示している。このふたつのクラスは、直線で簡単に分割できます（線形分割可能、lineary separable である）。\n","\n","左のグラフは、考え得る3 種類の線形分類器の決定境界を示しています。破線の決定境界を持つモデルは非常に性能が低く、クラスを正しく分割することさえできていません。ほかのふたつは、この訓練セットに対しては完璧に機能するが、決定境界がインスタンスに近いため、新しいインスタンスに対しても同じような性能を発揮することはできないだでしょう。\n","\n","それに対し、右側のグラフの実線は、SVM 分類器の決定境界を示しています。この線はふたつのクラスを分割できているだけでなく、もっとも近い訓練インスタンスからの距離ができる限り遠くなるようにしています。SVM 分類器は、クラスの間にできる限り太い道（2 本の平行な破線で表されている）を通すものだと考えることができる。これをマージンの大きい分類と呼びます。\n","「道から外れた」訓練インスタンスを増やしても、決定境界に影響は及ばないことに注意しましょう。\n","決定境界は、道の際にあるインスタンスによって決まります（サポートされる）。このようなインスタンスのことをサポートベクトル（support vector）と呼びます（右図で 大きな丸で描かれているもの）。"]},{"cell_type":"markdown","metadata":{"id":"XF09E2J7I3BG","colab_type":"text"},"source":["###特徴量のスケールから影響を受けやすいSVM"]},{"cell_type":"code","metadata":{"id":"uxdGkg_kHhm_","colab_type":"code","colab":{}},"source":["Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n","ys = np.array([0, 0, 1, 1])\n","svm_clf = SVC(kernel=\"linear\", C=100)\n","svm_clf.fit(Xs, ys)\n","\n","plt.figure(figsize=(12,3.2))\n","plt.subplot(121)\n","plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\n","plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\n","plot_svc_decision_boundary(svm_clf, 0, 6)\n","plt.xlabel(\"$x_0$\", fontsize=20)\n","plt.ylabel(\"$x_1$  \", fontsize=20, rotation=0)\n","plt.title(\"Unscaled\", fontsize=16)\n","plt.axis([0, 6, 0, 90])\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(Xs)\n","svm_clf.fit(X_scaled, ys)\n","\n","plt.subplot(122)\n","plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\n","plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\n","plot_svc_decision_boundary(svm_clf, -2, 2)\n","plt.xlabel(\"$x_0$\", fontsize=20)\n","plt.title(\"Scaled\", fontsize=16)\n","plt.axis([-2, 2, -2, 2])\n","\n","save_fig(\"sensitivity_to_feature_scales_plot\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XRRK8niIJB_w","colab_type":"text"},"source":["上図からSVM は、特徴量のスケールの影響を受けやすくあります。左側のグラフでは、縦方向のスケールが横方向のスケールよりもかなり大きいため、可能な道のなかでもっとも太いものはほとんど真横に向かうものになっています。特徴量をスケーリング（たとえば、\n","scikit-learn のStandardScaler で）したあとの決定境界（右側のグラフ）は、はるかによい感じに見えます。"]},{"cell_type":"markdown","metadata":{"id":"XfCjB7u-JN_q","colab_type":"text"},"source":["###ソフトマージン分類"]},{"cell_type":"markdown","metadata":{"id":"fIdGA3mDJVzv","colab_type":"text"},"source":["すべてのインスタンスが道に引っかからず、正しい側にいることを厳密に要求する場合、それをハードマージン分類（hard margine classification）と呼びます。ハードマージン分類には、データが\n","線形分割できるときでなければ使えず、外れ値に敏感になり過ぎるというふたつの大きな問題点があります。下図は、iris データセットに1 個の外れ値を追加したものを示しています。\n","\n","左側のグラフは、ハードマージンを見つけられないもの、右側のグラフは、外れ値のない最初に示した図とは決定境界がまったく異なり、おそらく同じようには汎化できないものであります。\n","これらの問題を避けるために、もっと柔軟性の高いモデルを使った方がよいと考えられます。\n","目標は、道をできる限り太くすることと、マージン違反（margin violation = 道のなかや間違った側に入ってしまう\n","インスタンス）を減らすこととの間でバランスを取ることであります。これをソフトマージン分類と呼びます。"]},{"cell_type":"code","metadata":{"id":"V_JbrUIBJxHa","colab_type":"code","colab":{}},"source":["X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\n","y_outliers = np.array([0, 0])\n","Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)\n","yo1 = np.concatenate([y, y_outliers[:1]], axis=0)\n","Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)\n","yo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n","\n","svm_clf2 = SVC(kernel=\"linear\", C=10**9)\n","svm_clf2.fit(Xo2, yo2)\n","\n","plt.figure(figsize=(12,2.7))\n","\n","plt.subplot(121)\n","plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\n","plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\n","plt.text(0.3, 1.0, \"Impossible!\", fontsize=24, color=\"red\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.annotate(\"Outlier\",\n","             xy=(X_outliers[0][0], X_outliers[0][1]),\n","             xytext=(2.5, 1.7),\n","             ha=\"center\",\n","             arrowprops=dict(facecolor='black', shrink=0.1),\n","             fontsize=16,\n","            )\n","plt.axis([0, 5.5, 0, 2])\n","\n","plt.subplot(122)\n","plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\n","plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\n","plot_svc_decision_boundary(svm_clf2, 0, 5.5)\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.annotate(\"Outlier\",\n","             xy=(X_outliers[1][0], X_outliers[1][1]),\n","             xytext=(3.2, 0.08),\n","             ha=\"center\",\n","             arrowprops=dict(facecolor='black', shrink=0.1),\n","             fontsize=16,\n","            )\n","plt.axis([0, 5.5, 0, 2])\n","\n","save_fig(\"sensitivity_to_outliers_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Us8CTLNgJ-FK","colab_type":"text"},"source":["scikit-learn のSVM クラス群では、C ハイパーパラメータでこのバランスを調整できます。C が小さければ小さいほど道は太くなるが、マージン違反も増えます。\n","下図は、線形分割できないデータセットに対して訓練したふたつのソフトマージンSVM 分類器の決定境界とマージンを示しています。左側のグラフは、C として大きな値を使った分類器で、マージン違反は少ないが、マージンが狭くなっています。\n","\n","右側のグラフは、C として小さな値を使った分類器で、マージンはかなり広いが、道に入り込んでいるインスタンスがかなり多きくなっています。しかし、第2 の分類器の方が汎化性能はよさそうに見えます。実際、この訓練セットでも、第2 の分類器の方が予測誤差は小さくあります。それは、ほとんどのマージン違反が実際には決定境界の正しい側に分類されるからであります。"]},{"cell_type":"code","metadata":{"id":"NtfJKBdiJxiL","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","\n","iris = datasets.load_iris()\n","X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n","y = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\n","\n","svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n","    ])\n","\n","svm_clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NATfOEKkKaWK","colab_type":"code","colab":{}},"source":["svm_clf.predict([[5.5, 1.7]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UELCh8h6Kc9v","colab_type":"code","colab":{}},"source":["scaler = StandardScaler()\n","svm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n","svm_clf2 = LinearSVC(C=100, loss=\"hinge\", random_state=42)\n","\n","scaled_svm_clf1 = Pipeline([\n","        (\"scaler\", scaler),\n","        (\"linear_svc\", svm_clf1),\n","    ])\n","scaled_svm_clf2 = Pipeline([\n","        (\"scaler\", scaler),\n","        (\"linear_svc\", svm_clf2),\n","    ])\n","\n","scaled_svm_clf1.fit(X, y)\n","scaled_svm_clf2.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMVBWF6QKgU1","colab_type":"code","colab":{}},"source":["# スケーリングされていないパラメーターに変換する\n","b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\n","b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n","w1 = svm_clf1.coef_[0] / scaler.scale_\n","w2 = svm_clf2.coef_[0] / scaler.scale_\n","svm_clf1.intercept_ = np.array([b1])\n","svm_clf2.intercept_ = np.array([b2])\n","svm_clf1.coef_ = np.array([w1])\n","svm_clf2.coef_ = np.array([w2])\n","\n","# サポートベクトルの検索\n","t = y * 2 - 1\n","support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\n","support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\n","svm_clf1.support_vectors_ = X[support_vectors_idx1]\n","svm_clf2.support_vectors_ = X[support_vectors_idx2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"atHMNZnXK6Oq","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(12,3.2))\n","plt.subplot(121)\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris-Virginica\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris-Versicolor\")\n","plot_svc_decision_boundary(svm_clf1, 4, 6)\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"upper left\", fontsize=14)\n","plt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\n","plt.axis([4, 6, 0.8, 2.8])\n","\n","plt.subplot(122)\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n","plot_svc_decision_boundary(svm_clf2, 4, 6)\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\n","plt.axis([4, 6, 0.8, 2.8])\n","\n","save_fig(\"regularization_plot\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xPFRfqPrK_dK","colab_type":"text"},"source":["scikit-learn のSVM クラス群では、C ハイパーパラメータでこのバランスを調整できます。C が小さければ小さいほど道は太くなるが、マージン違反も増えます。\n","\n","上図 は、線形分割できないデータセットに対して訓練したふたつのソフトマージンSVM 分類器の決定境界とマージンを示しています。左側のグラフは、C として大きな値を使った分類器で、マージン違反は少ないが、マージンが狭くなっています。右側のグラフは、C として小さな値を使った分類器で、マージンはかなり広いが、道に入り込んでいるインスタンスがかなり多くなります。しかし、第2 の分類器の方が汎化性能はよさそうに見えます。実際、この訓練セットでも、第2 の分類器の方が予測誤差は小さくなります。それは、ほとんどのマージン違反が実際には決定境界の正しい側に分類されるからであります。"]},{"cell_type":"markdown","metadata":{"id":"FRfAbh_nLgWz","colab_type":"text"},"source":["##非線形SVM分類器"]},{"cell_type":"markdown","metadata":{"id":"s7rhf3r8Livo","colab_type":"text"},"source":["線形SVM 分類器は効率的で多くの条件で驚くほどすばらしく機能するが、多くのデータセットは線形分割などとてもできません。このような非線形データセットを処理するためのアプローチのひとつは、多項式特徴量のように特徴量を追加するというものであります。実際、これで線形分割可能なデータセットが得られる場合があります。\n","\n","下図の左側のグラフを見てみましょう。これは、特徴量がひとつ（名前はx1）の単純なデータセットを表しているが、この単純なデータセットは線形分割不能であります。しかし、x2 = (x1)2 という第2 の特徴量を追加して得られた2 次元データセットは、完全に線形分割可能であります。"]},{"cell_type":"code","metadata":{"id":"D6NJ3ah4Lz0v","colab_type":"code","colab":{}},"source":["X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n","X2D = np.c_[X1D, X1D**2]\n","y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n","\n","plt.figure(figsize=(11, 4))\n","\n","plt.subplot(121)\n","plt.grid(True, which='both')\n","plt.axhline(y=0, color='k')\n","plt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\n","plt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\n","plt.gca().get_yaxis().set_ticks([])\n","plt.xlabel(r\"$x_1$\", fontsize=20)\n","plt.axis([-4.5, 4.5, -0.2, 0.2])\n","\n","plt.subplot(122)\n","plt.grid(True, which='both')\n","plt.axhline(y=0, color='k')\n","plt.axvline(x=0, color='k')\n","plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\n","plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\n","plt.xlabel(r\"$x_1$\", fontsize=20)\n","plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n","plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\n","plt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\n","plt.axis([-4.5, 4.5, -1, 17])\n","\n","plt.subplots_adjust(right=1)\n","\n","save_fig(\"higher_dimensions_plot\", tight_layout=False)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeFSVXLrL2TJ","colab_type":"text"},"source":["scikit-learn を使ってこの考え方を実装するには、PolynomialFeatures 変換器とそのあとにStandardScaler、LinearSVC を組み込んだPipeline を作るとよいです。これをmoons データセットでテストしてみましょう。"]},{"cell_type":"code","metadata":{"id":"DmUwXk9kL-qM","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n","\n","def plot_dataset(X, y, axes):\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n","    plt.axis(axes)\n","    plt.grid(True, which='both')\n","    plt.xlabel(r\"$x_1$\", fontsize=20)\n","    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n","\n","plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_FxiHKeMCF_","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_moons\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","polynomial_svm_clf = Pipeline([\n","        (\"poly_features\", PolynomialFeatures(degree=3)),\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n","    ])\n","\n","polynomial_svm_clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUOBWKN5MEiw","colab_type":"code","colab":{}},"source":["def plot_predictions(clf, axes):\n","    x0s = np.linspace(axes[0], axes[1], 100)\n","    x1s = np.linspace(axes[2], axes[3], 100)\n","    x0, x1 = np.meshgrid(x0s, x1s)\n","    X = np.c_[x0.ravel(), x1.ravel()]\n","    y_pred = clf.predict(X).reshape(x0.shape)\n","    y_decision = clf.decision_function(X).reshape(x0.shape)\n","    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n","    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n","\n","plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n","plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","\n","save_fig(\"moons_polynomial_svc_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj_IhMFyMUKh","colab_type":"text"},"source":["###多項式カーネル"]},{"cell_type":"markdown","metadata":{"id":"mL8ubGQuMYhT","colab_type":"text"},"source":["多項式特徴量を追加するのは実装が単純であり、あらゆる種類の機械学習アルゴリズム（SVMに限らず）ですばらしく機能するが、次数が低いと非常に複雑なデータセットを処理できず、次数\n","が高いと特徴量が膨大な数になってモデルが遅くなりすぎます。\n","\n","しかし、SVM を使う場合は、カーネルトリック（kernel trick）というテクニックを使うことができます。これを使うと、実際に特徴量を追加せずにまるで多くの多項式特徴量を追加したかのような結果が得られます。実際に特徴量を追加するわけではないため、特徴量数の組合せ爆発も生じません。このトリックは、SVC クラスによって実装されています。\n","moons データセットでテストしてみましょう。"]},{"cell_type":"code","metadata":{"id":"IhNGksHtMI1Y","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","\n","poly_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n","    ])\n","poly_kernel_svm_clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmA11JStMLO6","colab_type":"code","colab":{}},"source":["poly100_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n","    ])\n","poly100_kernel_svm_clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpPnBJ6KMNiJ","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(11, 4))\n","\n","plt.subplot(121)\n","plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n","plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","plt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n","\n","plt.subplot(122)\n","plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n","plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","plt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\n","\n","save_fig(\"moons_kernelized_polynomial_svc_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0umUlaNMysf","colab_type":"text"},"source":["上図は3 次元多項式カーネルでSVM 分類器を訓練した結果です。左側のグラフは、この分類器を表しています。右側のグラフには、10 次元多項式カーネルを使った別のSVM 分類器を示しています。\n","\n","モデルは過学習を起こしており、過学習している場合は、多項回帰モデルの次数を下げなければなりません。逆に、モデルが過小適合しているなら、多項回帰モデルの次数を上げることになります。ハイパーパラメータのcoef0 で、高次多項式モデルと低次多項式モデルからどの程度の影響を認めるかを調節できます。"]},{"cell_type":"markdown","metadata":{"id":"dDdxxRsCNS4S","colab_type":"text"},"source":["###ガウスRBF カーネル"]},{"cell_type":"markdown","metadata":{"id":"ZYuTYVpENVlO","colab_type":"text"},"source":["多項式特徴量の方式と同様に、類似性特徴量の方式はどの機械学習アルゴリズムでも使えるが、特に訓練セットが大きい場合には、すべての追加特徴量を計算していると、計算量という面でコス\n","トが高くなってしまう場合があります。\n","\n","しかし、SVM では、ここでもカーネルトリックが威力を発揮する。実際に類似性特徴量を追加しなくても、多数の類似性特徴量を追加したのと同じ結果が得られるのであります。SVC クラスでガウスRBF カーネルを試してみましょう。"]},{"cell_type":"code","metadata":{"id":"SZ1V89nbNiig","colab_type":"code","colab":{}},"source":["rbf_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n","    ])\n","rbf_kernel_svm_clf.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpke0O7-NsHk","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","\n","gamma1, gamma2 = 0.1, 5\n","C1, C2 = 0.001, 1000\n","hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n","\n","svm_clfs = []\n","for gamma, C in hyperparams:\n","    rbf_kernel_svm_clf = Pipeline([\n","            (\"scaler\", StandardScaler()),\n","            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n","        ])\n","    rbf_kernel_svm_clf.fit(X, y)\n","    svm_clfs.append(rbf_kernel_svm_clf)\n","\n","plt.figure(figsize=(11, 7))\n","\n","for i, svm_clf in enumerate(svm_clfs):\n","    plt.subplot(221 + i)\n","    plot_predictions(svm_clf, [-1.5, 2.5, -1, 1.5])\n","    plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","    gamma, C = hyperparams[i]\n","    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n","\n","save_fig(\"moons_rbf_svc_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHzAR6EdNyM7","colab_type":"text"},"source":["ガウスRBF カーネルモデルは、左下に示してある。ほかのグラフは、ハイパーパラメータのgammaとC の値を変えて訓練したモデルであります。gamma を増やすと、ベル型の曲線が狭くなり、その結果、各インスタンスの影響を受ける範囲が小さくなります。決定境界\n","は不規則になり、各インスタンスの周囲でくねくねと曲がります。\n","\n","逆に、gamma を小さくすると、ベル形の曲線の幅が広くなり、各インスタンスの影響を受ける範囲が広がり、決定境界は滑らかになります。つまり、 は正則化ハイパーパラメータと同じように機能します。モデルが過学習しているときにはgammma を小さくし、過小適合しているときには を大きくするとよいといえます（同じことがC ハイパーパ\n","ラメータにも当てはまる）。\n","\n","カーネルはほかにもあるが、RBF カーネルと比べてごくまれにしか使われません。たとえば、特定のデータ構造に専門特化したカーネルがあります。テキストやDNA シーケンスの分類では、文字列カーネル（string kernel）が使われることがあります（たとえば、String Subsequence Kernel やレーベンシュタイン距離：Levenshtein distance に基づくカーネル）。"]},{"cell_type":"markdown","metadata":{"id":"P7P3EdTdObY_","colab_type":"text"},"source":["選べるカーネルがたくさんあるなかで、どれを選んだらよいか、目安として、特に訓練セットが非常に大きい場合や特徴量がたくさんあるときには、まず線形カーネルを選ぶようにしましょう（SVC(kernel=\"linear\") よりもLinearSVC の方がはるかに高速だとい\n","うことを覚えておきたい）。\n","\n","訓練セットがそれほど大きくないときには、ガウスRBF カーネルも試してみてもよいです。ほとんどの場合はうまく機能します。そして、時間と計算能力に余裕があり、特に訓練セットのデータ構造に対する専門のカーネルがある場合には、交差検証とグリッドサーチを使ってほかのカーネルを試してみてもよいです。"]},{"cell_type":"markdown","metadata":{"id":"j3otsmnFO06Y","colab_type":"text"},"source":["##(参考)SVM回帰"]},{"cell_type":"markdown","metadata":{"id":"u_b07BTKO5m_","colab_type":"text"},"source":["SVM アルゴリズムは柔軟性が高く、線形、非線形分類をサポートするだ\n","けでなく、線形、非線形回帰もサポートします。ポイントは、目的を逆にすることであります。マージン違反を減らしながらふたつのクラスの間にもっとも太い道を通すのではなく、SVM 回帰はマージン違\n","反を減らしながら道のなかに入るインスタンスができる限り多くなるようにします（この場合のマージン違反は、道に入っていないことである）。\n","\n","道の太さは、ハイパーパラメータ\" によって調節されます。下図は、無作為な線形データに対して訓練したふたつの線形SVM 回帰モデルを示しています。\n","片方はマージンが大きく（\" = 1.5）、もう片方はマージンが小さいモデルです（\" = 0.5）。\n","マージンに入る訓練インスタンスを増やしても、モデルの予測に影響はありません。そのようなことから、このモデルは、ε不感（ε-insensitive）だと言われています。\n","\n","\n","scikit-learn のLinearSVR クラスを使えば、線形SVM 回帰を行うことができます。次のコードは、の左側のグラフが表すモデルを作ります（まず、訓練データをスケーリングして、中央に移動しなければならない）。"]},{"cell_type":"code","metadata":{"id":"dP9OAWULPXFk","colab_type":"code","colab":{}},"source":["np.random.seed(42)\n","m = 50\n","X = 2 * np.random.rand(m, 1)\n","y = (4 + 3 * X + np.random.randn(m, 1)).ravel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyNI2kUdPY-8","colab_type":"code","colab":{}},"source":["from sklearn.svm import LinearSVR\n","\n","svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n","svm_reg.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiWtDPf_Pamh","colab_type":"code","colab":{}},"source":["svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\n","svm_reg2 = LinearSVR(epsilon=0.5, random_state=42)\n","svm_reg1.fit(X, y)\n","svm_reg2.fit(X, y)\n","\n","def find_support_vectors(svm_reg, X, y):\n","    y_pred = svm_reg.predict(X)\n","    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)\n","    return np.argwhere(off_margin)\n","\n","svm_reg1.support_ = find_support_vectors(svm_reg1, X, y)\n","svm_reg2.support_ = find_support_vectors(svm_reg2, X, y)\n","\n","eps_x1 = 1\n","eps_y_pred = svm_reg1.predict([[eps_x1]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5LCCtgVNPfDC","colab_type":"code","colab":{}},"source":["def plot_svm_regression(svm_reg, X, y, axes):\n","    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n","    y_pred = svm_reg.predict(x1s)\n","    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n","    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n","    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n","    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n","    plt.plot(X, y, \"bo\")\n","    plt.xlabel(r\"$x_1$\", fontsize=18)\n","    plt.legend(loc=\"upper left\", fontsize=18)\n","    plt.axis(axes)\n","\n","plt.figure(figsize=(9, 4))\n","plt.subplot(121)\n","plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])\n","plt.title(r\"$\\epsilon = {}$\".format(svm_reg1.epsilon), fontsize=18)\n","plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n","#plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], \"k-\", linewidth=2)\n","plt.annotate(\n","        '', xy=(eps_x1, eps_y_pred), xycoords='data',\n","        xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),\n","        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}\n","    )\n","plt.text(0.91, 5.6, r\"$\\epsilon$\", fontsize=20)\n","plt.subplot(122)\n","plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\n","plt.title(r\"$\\epsilon = {}$\".format(svm_reg2.epsilon), fontsize=18)\n","save_fig(\"svm_regression_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sy8X4DDzPkkj","colab_type":"text"},"source":["非線形回帰には、カーネル化されたSVM モデルを使えばよいです。たとえば、下図は、無作為な二次回帰訓練セットに対する二次多項式カーネルを使ったSVM 回帰を示しています。\n","\n","左側のグラフはほとんど正則化されていない（つまりC の値が大きい）が、右側のグラフはかなり正則化され\n","ています（つまりC の値が小さい）。\n","次のコードは、scikit-learn のSVR クラス（カーネルトリックをサポートする）を使って、図の左側のモデルを作ります。SVR クラスはSVC クラスの回帰版と言うべきもので、同じようにLinearSVR クラスはLinearSVC クラスの回帰版であります。LinearSVR クラスは訓練セット\n","に対して線形にスケーリングする（LinearSVC クラスと同様に）が、SVR クラスは訓練セットが大きくなるとそれ以上に非常に遅くなります（SVC クラスと同様に）。"]},{"cell_type":"code","metadata":{"id":"E2uIaT7cPy0Z","colab_type":"code","colab":{}},"source":["np.random.seed(42)\n","m = 100\n","X = 2 * np.random.rand(m, 1) - 1\n","y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdtHvdgrP2in","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVR\n","\n","svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n","svm_poly_reg.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUzxpBM7P5Ay","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVR\n","\n","svm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n","svm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1, gamma=\"auto\")\n","svm_poly_reg1.fit(X, y)\n","svm_poly_reg2.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhrBbevcP63P","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(9, 4))\n","plt.subplot(121)\n","plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\n","plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)\n","plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n","plt.subplot(122)\n","plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n","plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)\n","save_fig(\"svm_with_polynomial_kernel_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]}]}