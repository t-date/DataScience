{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"4_05_ensemble_learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XQhZsl486mlO","colab_type":"text"},"source":["#4-5 アンサンブル学習"]},{"cell_type":"markdown","metadata":{"id":"94o6nyCAaTeC","colab_type":"text"},"source":["## 概要"]},{"cell_type":"markdown","metadata":{"id":"rVeR2ov16mlS","colab_type":"text"},"source":["**アンサンブル学習は、性能の低い学習器（弱学習機）を組み合わせて、性能の高い学習器を作る方法です。**\n","\n","アンサンブル学習の代表的な手法として、「バギング」と「ブースティング」があります。\n"]},{"cell_type":"markdown","metadata":{"id":"5oHYCCnRQ83I","colab_type":"text"},"source":["###バギング"]},{"cell_type":"markdown","metadata":{"id":"3slsQRtXQ_ME","colab_type":"text"},"source":["「バギング」は、弱学習器を並列に学習して組み合わせる手法です。\n","元データから、重複を許してランダムにデータを取得（ブーストラップ）し、並列に学習して、多数決で分類結果を出力します。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_05_01.jpg?raw=true\" width=\"550px\">\n","\n","「バギング」の代表的な手法として、「ランダムフォレスト」があります。「ランダムフォレスト」は、弱学習器として決定木を使い、複数の決定木を使うことで精度向上を図ります。"]},{"cell_type":"markdown","metadata":{"id":"OHoOZf9mSBsJ","colab_type":"text"},"source":["###ブースティング"]},{"cell_type":"markdown","metadata":{"id":"1PgNQtvRSFLC","colab_type":"text"},"source":["「ブースティング」は、弱学習器を順番に学習して組み合わせて強くしていく手法です。\n","前の学習器が誤分類したデータを優先的に正しく分類できるように学習していきます。\n","\n","<img src=\"https://raw.githubusercontent.com/t-date/DataScience/master/fig/04_05_02.jpg?raw=true\" width=\"550px\">"]},{"cell_type":"markdown","metadata":{"id":"OWqQCd2rSa6S","colab_type":"text"},"source":["「ブースティング」の代表的な手法として、「勾配ブースティング」があります。\n","勾配ブースティングでは、損失関数として、勾配降下法を使用しています。「勾配ブー\n","スティング」を高速に実行できるようにC++ で開発した「XGBoost」が有名です。"]},{"cell_type":"markdown","metadata":{"id":"PdoluBvnTu-g","colab_type":"text"},"source":["##バギングとペースティング"]},{"cell_type":"markdown","metadata":{"id":"lfysK4UgTwml","colab_type":"text"},"source":["多様性の高い分類器を用意するための方法のひとつは、大きく異なる訓練アルゴリズムを使うことだが、すべての分類器で同じ訓練アルゴリズムを使いつつ、訓練セットから無作為に別々のサブセットをサンプリングして訓練するというアプローチがあります。サンプリングが重複ありで行われるときにはバギング（bagging、bootstrap\n","aggreating の略。）、重複なしで行われるときにはペースティングと呼びます。\n","\n","言い換えると、バギングとペースティングはともに複数の予測器が同じ訓練インスタンスを複数回サンプリングすることを認めるが、同じ予測器が同じ訓練インスタンスを複数回サンプリングすることを認めるのはバギングだけであります。概要の図 は、このようなサンプリングと訓練のプロセスを描いたものであります。\n","\n","すべての分類器を予測したら、アンサンブルは単純にすべての予測器の予測を集計して新インスタンスに対する予測をすることができます。集計関数は、一般に分類ではモード（statistical mode、\n","つまりハード投票分類器と同様に、予測の最頻値を取る）、回帰では平均である。個別の予測器は、訓練セット全体を対象として訓練したときよりもバイアスが高くなっているが、集計によってバイ\n","アスと分散の両方が下がります。一般に、もとの訓練セット全体でひとつの予測器を訓練したときと比べて、アンサンブルでは、バイアスは同じようなものだが、分散は下がっています。\n","\n","異なるCPU コアや異なるサーバーを使ってすべての分類器を並列に訓\n","練できる。同様に、予測も並列に実行できます。バギングやペースティングの人気が高い理由のひとつがこのスケーラビリティの高さであります。"]},{"cell_type":"markdown","metadata":{"id":"jk_GVqOuUzi-","colab_type":"text"},"source":["###scikit-learn におけるバギングとペースティング"]},{"cell_type":"markdown","metadata":{"id":"GgbSFT4MU2Ag","colab_type":"text"},"source":["scikit-learn は、バギングとペースティングの両方に対してBaggingClassifier クラスという単純なAPI を提供しています（回帰の場合は、BaggingRegressor クラス）。次のコードは、500 個の決定木分類器によるアンサンブルを訓練します。個々の分類器は、重複ありで訓練セットから100 個の訓練インスタンスを無作為抽出します。これはバギングの例だが、ペースティングを使いたい場合はbootstrap=False を設定すればよいです。n_jobs パラメータは、scikit-learn に訓\n","練、予測のために使うCPU コアの数を指示します（−1 にすると、scikit-learn は使えるすべてのコアを使う）。"]},{"cell_type":"code","metadata":{"id":"t-g0gsaDVPfI","colab_type":"code","colab":{}},"source":["# Python 2, 3 をサポートします\n","from __future__ import division, print_function, unicode_literals\n","\n","# 標準ライブラリのインポート\n","import numpy as np\n","import os\n","\n","# 乱数の固定\n","np.random.seed(42)\n","\n","# プロットの設定\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# 保存先ディレクトリの設定\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"ensembles\"\n","IMAGE_DIR = \"images\"\n","\n","os.makedirs(os.path.join(PROJECT_ROOT_DIR, IMAGE_DIR, CHAPTER_ID), exist_ok=True)\n","\n","def image_path(fig_id):\n","    return os.path.join(PROJECT_ROOT_DIR, IMAGE_DIR, CHAPTER_ID, fig_id)\n","\n","def save_fig(fig_id, tight_layout=True):\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGf0xU5IVg_D","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7LSx9FiVEAE","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","bag_clf = BaggingClassifier(\n","    DecisionTreeClassifier(random_state=42), n_estimators=500,\n","    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n","bag_clf.fit(X_train, y_train)\n","y_pred = bag_clf.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuOWlSRzVGSC","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lc5E-brDVsZa","colab_type":"code","colab":{}},"source":["tree_clf = DecisionTreeClassifier(random_state=42)\n","tree_clf.fit(X_train, y_train)\n","y_pred_tree = tree_clf.predict(X_test)\n","print(accuracy_score(y_test, y_pred_tree))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtCROvrWVu5z","colab_type":"code","colab":{}},"source":["from matplotlib.colors import ListedColormap\n","\n","def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n","    x1s = np.linspace(axes[0], axes[1], 100)\n","    x2s = np.linspace(axes[2], axes[3], 100)\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n","    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n","    if contour:\n","        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n","        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n","    plt.axis(axes)\n","    plt.xlabel(r\"$x_1$\", fontsize=18)\n","    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ep0JGwIiVxZS","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(11,4))\n","plt.subplot(121)\n","plot_decision_boundary(tree_clf, X, y)\n","plt.title(\"Decision Tree\", fontsize=14)\n","plt.subplot(122)\n","plot_decision_boundary(bag_clf, X, y)\n","plt.title(\"Decision Trees with Bagging\", fontsize=14)\n","save_fig(\"decision_tree_without_and_with_bagging_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DCCXHacMWNez","colab_type":"text"},"source":["上図は、ひとつの決定木を使ったときの決定境界と500 個の木によるバギングアンサンブル（上記のコード）を使ったときの決定境界を比較したものであります。ご覧のようにアンサンブルの予\n","測は、単独の決定木による予測よりもはるかに汎化性能が高くなります。アンサンブルのバイアスはほぼ同じだが、分散は小さくなります（訓練セットでの予測誤りはほぼ同じだが、決定境界の不規則度は下がる）。\n","\n","ブートストラップ法により、個々の予測器が訓練に使うサブセットの多様性が若干上がるため、バギングの方がペースティングよりもバイアスが少し高くなるが、それは予測器の相関が下がると\n","いうことであるため、アンサンブルの分散は下がります。全体として、バギングの方がペースティングより\n","もよいモデルになることが多くあります。一般に、バギングの方が好まれているのはそのためであります。しかし、時間とCPU パワーに余裕がある場合には、バギングとペースティングを交差検証して、性能\n","のよい方を選ぶことができます。"]},{"cell_type":"markdown","metadata":{"id":"_HVF_BQBYBo8","colab_type":"text"},"source":["###ランダムフォレスト"]},{"cell_type":"markdown","metadata":{"id":"l5tNrYxDX9N7","colab_type":"text"},"source":["ランダムフォレストは、決定木のアンサンブルで、一般にバギングメソッドで訓練され（ペースティングが使われる場合もある）、\n","max_samples は訓練セットサイズに設定されます。BaggingClassifier を構築し、それをDecisionTreeClassifier に渡さなくても、RandomForestClassifier クラスを使えるようになっています。\n","\n","RandomForestClassifier クラスの方が便利なだけでなく、決定木に最適化されています（同様に、回帰のタスクのためにRandomForestRegressor クラスが用意されている）。\n","\n","次のコードは、500 個の木（それぞれ最大16ノードに制限されている）によるランダムフォレスト分類器を訓練します。\n","\n","\n","RandomForestClassifier は、少数の例外を除き、木をどのように育てるかを調整するためにDecisionTreeClassifier のハイパーパラメータをすべて持つほか、アンサンブル自体を調整するためにBaggingClassifier のハイパーパラメータもすべて持っています。\n","\n","\n","ランダムフォレストアルゴリズムは、木を育てるときにさらに無作為性を生み出します。ノードを分\n","割するときに最良の特徴量を探すのではなく、特徴量の無作為なサブセットから最良の特徴量を探します。その分、木の多様性は増し、それにより（繰り返しになるが）バイアスが上がる分、\n","分散が下がって、全体としてよりよいモデルが作られます。次のBaggingClassifier は、前のRandomForestClassifier とほぼ同じであります。"]},{"cell_type":"code","metadata":{"id":"4jmidNYkYOwE","colab_type":"code","colab":{}},"source":["bag_clf = BaggingClassifier(\n","    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n","    n_estimators=500, max_samples=1.0, bootstrap=True, random_state=42)#n_jobs=-1, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLyeU5MkYfNH","colab_type":"code","colab":{}},"source":["bag_clf.fit(X_train, y_train)\n","y_pred = bag_clf.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8X2pLbXcYkA0","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42) # n_jobs=-1, random_state=42)\n","rnd_clf.fit(X_train, y_train)\n","\n","y_pred_rf = rnd_clf.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"st_BQCMUYtUI","colab_type":"code","colab":{}},"source":["np.sum(y_pred == y_pred_rf) / len(y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5O92wRf4ZTNx","colab_type":"text"},"source":["ランダムフォレストの別の優れた点は、各特徴量の相対的な重要度を簡単に知ることができることであります。scikit-learn は、その特徴量を使うノードが平均して（フォレスト内のすべての木にわ\n","たり）不純度をどれくらい減らすかを調べることにより、特徴量の重要性を測ります。\n","scikit-learn は、訓練の後に各特徴量に対してこのスコアを自動的に計算し、その後、すべ\n","ての重要度の合計が1 になるように結果をスケールします。この結果に得るには、feature_\n","importances_変数を使用します。たとえば、次のコードは、iris データセットのRandomForestClassifier を訓練し、各機能の重要性を出力します。もっとも重要な特徴量は\n","花弁の長さ（44%）と幅（42%）で、それと比べるとがく片の長さと幅はそれほど重要ではないようすがわかります（それぞれ11% と2%）。"]},{"cell_type":"code","metadata":{"id":"uIF0RBENYvjP","colab_type":"code","colab":{}},"source":["from sklearn.datasets import load_iris\n","iris = load_iris()\n","rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n","rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n","for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n","    print(name, score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJOlrBR9YyWn","colab_type":"code","colab":{}},"source":["rnd_clf.feature_importances_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbRSbuGbY0Jm","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(6, 4))\n","\n","for i in range(15):\n","    tree_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i)\n","    indices_with_replacement = np.random.randint(0, len(X_train), len(X_train))\n","    tree_clf.fit(X[indices_with_replacement], y[indices_with_replacement])\n","    plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.02, contour=False)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tZtRjPiZlpA","colab_type":"text"},"source":["ランダムフォレストは、特に特徴量選択が必要なときに、どの特徴量が本当の意味で重要かを手っ取り早く調べるために便利であります。"]},{"cell_type":"markdown","metadata":{"id":"sv3PUMwsZoH4","colab_type":"text"},"source":["##ブースティング"]},{"cell_type":"markdown","metadata":{"id":"Gv-UIFU7ZqM6","colab_type":"text"},"source":["ブースティング（boosting）は、複数の弱学習器を結合して強学習器を作れるあらゆるアンサンブルメソッドを指します。\n","ほとんどのブースティングメソッドの一般的な考え方は、逐次的に予測器を訓練し、それによって直前の予測器の修正を試みるというものであります。ブースティングメソッドは多数あるが、人気があるのは、アダブースト（AdaBoost、エイダブーストとも呼ばれています。Adaptive Boosting= 適応的ブースティングの略）と勾配ブースティング（GradientBoosting）である。アダブーストから見ていきましょう。"]},{"cell_type":"markdown","metadata":{"id":"-ypj5kMAZ6a5","colab_type":"text"},"source":["###アダブースティング"]},{"cell_type":"markdown","metadata":{"id":"jxpe8zxeaCbx","colab_type":"text"},"source":["新しい予測器が前の予測器を修正する方法のひとつとしては、前の予測器が過小適合した訓練インスタンスに少しだけ余分に注意を払うというものがあります。そうすると、少しずつ難しい条件につ\n","いてよく学習した予測器が生まれる。これがアダブーストのテクニックであります。\n","\n","\n","たとえばアダブースト分類器を作る場合、まずベースの分類器（決定木など）を訓練し、訓練\n","セットを対象として予測のために使います。そして、分類に失敗した訓練インスタンスの相対的な重み\n","を上げる。次に、更新された重みを使って第2 の分類器を訓練し、予測に使って、重みを更新す\n","る。これを繰り返していきます。\n","\n","下図は、moons データセットを使って連続的に訓練した5 つの予測器の決定境界を示しています（この例では、個々の予測器は、RBF カーネルで高度に正則化されたSVM 分類器である）。\n","最初の分類器は多くのインスタンスで分類ミスを犯しているので、それらのインスタンスの重みが上げられています。そのため、第2 の分類器は、それらの分類器で性能が上がっています。第5 の分類\n","器まで、それが連続しています。\n","\n","右側のグラフは、学習率を半分にしています（つまり、分類ミスを犯したインスタンスに対して各イテレーションが与える重みを半分にしている）。ご覧のように、こ\n","の逐次的な学習テクニックは、勾配降下法と似ているが、コスト関数を最小化するためにひとつの予測器のパラメータを操作するのではなく、アンサンブルに予測器を追加してアンサンブルを少しずつ改良していきます。"]},{"cell_type":"code","metadata":{"id":"s1E8f76racT0","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.svm import SVC\n","\n","ada_clf = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n","    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n","ada_clf.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr8iBv04ae4S","colab_type":"code","colab":{}},"source":["plot_decision_boundary(ada_clf, X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjAiIWHdaiIP","colab_type":"code","colab":{}},"source":["m = len(X_train)\n","\n","plt.figure(figsize=(11, 4))\n","for subplot, learning_rate in ((121, 1), (122, 0.5)):\n","    sample_weights = np.ones(m)\n","    plt.subplot(subplot)\n","    for i in range(5):\n","        svm_clf = SVC(kernel=\"rbf\", C=0.05, gamma=\"auto\", random_state=42)\n","        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n","        y_pred = svm_clf.predict(X_train)\n","        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n","        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n","        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n","    if subplot == 121:\n","        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n","        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n","        plt.text(-0.5,  0.10, \"3\", fontsize=14)\n","        plt.text(-0.4,  0.55, \"4\", fontsize=14)\n","        plt.text(-0.3,  0.90, \"5\", fontsize=14)\n","\n","save_fig(\"boosting_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVi73PLoavrr","colab_type":"text"},"source":["###勾配ブースティング"]},{"cell_type":"markdown","metadata":{"id":"e0Iq3hrYayjU","colab_type":"text"},"source":["勾配ブースティング（gradient boosting）もよく使われているブース\n","ティングアルゴリズムであります。勾配ブースティングも、アダブーストと同様に、アンサンブルに前の予測器を改良した予測器を逐次的に加えていきます。しかし、アダブーストのようにイテレー\n","ションごとにインスタンスの重みを調整するのではなく、新予測器を前の予測器の残差（residual\n","error）に適合させようとします。\n","\n","\n","では、ベース予測器として決定木を使った単純な回帰の例を見てみましょう（勾配ブースティングは、\n","もちろん回帰のタスクでもしっかりと機能する）。これを勾配ブースティング木（gradient tree\n","boosting）とか勾配ブースティング決定木（GBRT）（gradient boosted regression tree）と呼びます。\n","まず、訓練セット（たとえば、ノイズのある2 次関数訓練セット）にDecisionTreeRegressorを適合させましょう。"]},{"cell_type":"code","metadata":{"id":"fi1IIqmYbBJG","colab_type":"code","colab":{}},"source":["np.random.seed(42)\n","X = np.random.rand(100, 1) - 0.5\n","y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVoIJ5TnbCkK","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n","tree_reg1.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6IzBtBC_bVL9","colab_type":"text"},"source":["次に、第1 の予測器が作った残差を使って第2 のDecisionTreeRegressor を訓練します。"]},{"cell_type":"code","metadata":{"id":"G4I-IGihbEOb","colab_type":"code","colab":{}},"source":["y2 = y - tree_reg1.predict(X)\n","tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n","tree_reg2.fit(X, y2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Y0wWs3rbay1","colab_type":"text"},"source":["そして、第2 の予測器が作った残差を使って第3 の回帰器を訓練します。"]},{"cell_type":"code","metadata":{"id":"QD5RW-g9bGKj","colab_type":"code","colab":{}},"source":["y3 = y2 - tree_reg2.predict(X)\n","tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n","tree_reg3.fit(X, y3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoEiwpqhbIAl","colab_type":"code","colab":{}},"source":["X_new = np.array([[0.8]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skVTpLTrbet8","colab_type":"text"},"source":["これで3 つの木を含むアンサンブルができた。このアンサンブルは、すべての木の予測を単純に加算するという形で新しいインスタンスの予測をすることができます。"]},{"cell_type":"code","metadata":{"id":"SWrv1W8cbJop","colab_type":"code","colab":{}},"source":["y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uC0IEbhzbLQX","colab_type":"code","colab":{}},"source":["y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QEv1CAcbQKS","colab_type":"code","colab":{}},"source":["def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n","    x1 = np.linspace(axes[0], axes[1], 500)\n","    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n","    plt.plot(X[:, 0], y, data_style, label=data_label)\n","    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n","    if label or data_label:\n","        plt.legend(loc=\"upper center\", fontsize=16)\n","    plt.axis(axes)\n","\n","plt.figure(figsize=(11,11))\n","\n","plt.subplot(321)\n","plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n","plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n","plt.title(\"Residuals and tree predictions\", fontsize=16)\n","\n","plt.subplot(322)\n","plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n","plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n","plt.title(\"Ensemble predictions\", fontsize=16)\n","\n","plt.subplot(323)\n","plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n","plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n","\n","plt.subplot(324)\n","plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n","plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n","\n","plt.subplot(325)\n","plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n","plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n","plt.xlabel(\"$x_1$\", fontsize=16)\n","\n","plt.subplot(326)\n","plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n","plt.xlabel(\"$x_1$\", fontsize=16)\n","plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n","\n","save_fig(\"gradient_boosting_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YF6sSXKAbjqP","colab_type":"text"},"source":["上図は、左側にこれら3 つの決定木の予測、右側にアンサンブルの予測を示しています。第1 行\n","では、アンサンブルに含まれているのは1 つの決定木だけであるため、アンサンブルの予測は第1 の\n","決定木の予測とまったく同じであります。第2 行では、第1 の決定木の残差を対象として新しい決定\n","木を訓練しています。アンサンブルの予測は、最初のふたつの決定木の輪だということがわかるはず\n","です。新たな決定木を追加するたびに、アンサンブルの予測が次第に改善されていくことがわかります。"]},{"cell_type":"markdown","metadata":{"id":"XeUM9k-rbzk9","colab_type":"text"},"source":["scikit-learn のGradientBoostingRegressor クラスを使えば、もっと簡単にGBRT アンサンブルを訓練できます。RandomForestRegressor クラスと同様に、このクラスには決定木の\n","成長を調整するハイパーパラメータ（max_depth、min_samples_leaf など）とアンサンブ\n","ル訓練を調整するハイパーパラメータ（n_estimators）がある。次のコードは、先ほどと同じアンサンブルを作ります。"]},{"cell_type":"code","metadata":{"id":"s3hX_sjGcAMV","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n","gbrt.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6RH7lnVcDKT","colab_type":"code","colab":{}},"source":["gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n","gbrt_slow.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHs2og0-cVQ8","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(11,4))\n","\n","plt.subplot(121)\n","plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n","plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n","\n","plt.subplot(122)\n","plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n","plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n","\n","save_fig(\"gbrt_learning_rate_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-s6SG7yHcIpQ","colab_type":"text"},"source":["learning_rate ハイパーパラメータは、個々の木の影響力を調整します。0.1 などの低い値に設定すると、訓練セットへの適合のためにアンサンブルに多くの決定木を追加しなければならなく\n","なるが、通常は予測の汎化性能が上がる。これは、収縮（shrinkage）という正則化テクニックで\n","あります。\n","\n","上図は、低い学習率で訓練したふたつのGBRT アンサンブルを示しています。左側は決\n","定木の数が少なすぎて訓練セットに適合できていないのに対し、右側は決定木が多すぎて訓練セッ\n","トに過学習している。"]},{"cell_type":"markdown","metadata":{"id":"vKiFVix0cmKS","colab_type":"text"},"source":["決定木の最適な数を知るためには、早期打ち切りを使えばよいです。staged_predict() メソッドを使えば、簡単に実装できます。このメソッドは、訓練の各ステージ（決\n","定木が1 個、決定木が2 個……）でアンサンブルが行った予測を示す反復子を返します。次のコードは、\n","120 個の決定木でGBRT アンサンブルを訓練し、訓練の各ステージで検証誤差を測定して決定木\n","の最適な数を調べ、最後にその最適な数の決定木を使って別のGBRT アンサンブルを訓練します。"]},{"cell_type":"code","metadata":{"id":"Dkk5ycJscr2V","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n","\n","gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n","gbrt.fit(X_train, y_train)\n","\n","errors = [mean_squared_error(y_val, y_pred)\n","          for y_pred in gbrt.staged_predict(X_val)]\n","bst_n_estimators = np.argmin(errors) + 1\n","\n","gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)\n","gbrt_best.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tvmuv3mdcvZW","colab_type":"code","colab":{}},"source":["min_error = np.min(errors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXUVz3Mwcxgv","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(11, 4))\n","\n","plt.subplot(121)\n","plt.plot(errors, \"b.-\")\n","plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n","plt.plot([0, 120], [min_error, min_error], \"k--\")\n","plt.plot(bst_n_estimators, min_error, \"ko\")\n","plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n","plt.axis([0, 120, 0, 0.01])\n","plt.xlabel(\"Number of trees\")\n","plt.title(\"Validation error\", fontsize=14)\n","\n","plt.subplot(122)\n","plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n","plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n","\n","save_fig(\"early_stopping_gbrt_plot\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvloyPIBc8_c","colab_type":"text"},"source":["実際に訓練を早い段階で打ち切って早期打ち切りを実現することもできます（最初に大量の決定木\n","を訓練してから、最適な個数を探すのではなく）。そのためには、warm_start=True を設定し\n","て、fit() メソッドを呼び出したときにscikit-learn が既存の決定木を残して、漸進的に訓練を実\n","行できるようにします。次のコードは、検証誤差が5 回連続で改善されないときに訓練を打ち切ります。"]},{"cell_type":"code","metadata":{"id":"dhgdUEHEdBMv","colab_type":"code","colab":{}},"source":["gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n","\n","min_val_error = float(\"inf\")\n","error_going_up = 0\n","for n_estimators in range(1, 120):\n","    gbrt.n_estimators = n_estimators\n","    gbrt.fit(X_train, y_train)\n","    y_pred = gbrt.predict(X_val)\n","    val_error = mean_squared_error(y_val, y_pred)\n","    if val_error < min_val_error:\n","        min_val_error = val_error\n","        error_going_up = 0\n","    else:\n","        error_going_up += 1\n","        if error_going_up == 5:\n","            break  # early stopping"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7rXpocFdFQ8","colab_type":"code","colab":{}},"source":["print(gbrt.n_estimators)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGlpeEPEdHOR","colab_type":"code","colab":{}},"source":["print(\"Minimum validation MSE:\", min_val_error)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IyikmfVndL2c","colab_type":"text"},"source":["GradientBoostingRegressor クラスは、個々の決定木を訓練するために使われる訓練イ\n","ンスタンスの割合を指定するsubsample ハイパーパラメータもサポートしています。たとえば、\n","subsample=0.25 とすると、個々の決定木は無作為に選択された25% の訓練インスタンスを\n","使って訓練される。これはバイアスを少し上げた分、分散を\n","下げます。また、訓練のスピードもかなり上がります。このテクニックを確率的勾配ブースティング\n","（stochastic gradient boosting）と呼びます。"]}]}