{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-08_clustering.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"imgDjRpBGlke","colab_type":"text"},"source":["#4-8 クラスタリング"]},{"cell_type":"markdown","metadata":{"id":"z32STcVeH5T5","colab_type":"text"},"source":["##概要\n","**クラスタリングは、データなどの集合体を機能やカテゴリごとに分けて集める手法です。**"]},{"cell_type":"markdown","metadata":{"id":"a4jheVRPh8I8","colab_type":"text"},"source":["## クラスタリング(教師あり学習)\n","教師あり学習におけるクラスタリングとして、k 近傍法\n","（k-Nearest Neighbor :k-NN）があります。K近傍法と、k-Means法（K-平均法）とは別物だということに注意してください。"]},{"cell_type":"markdown","metadata":{"id":"32tZKLDGMLSn","colab_type":"text"},"source":["###k 近傍法（k-NN法）"]},{"cell_type":"markdown","metadata":{"id":"T3UGC9Txn6KB","colab_type":"text"},"source":["**k 近傍法は、あらかじめクラス分けされた教師データをもとに、新しいデータのクラスを最も近いk 個データのクラスから多数決で分類する手法です。**\n","\n","オブジェクトの分類を、その近傍のオブジェクト群の投票によって決定します。\n","図は、K=3 及び k=5 の例です。\n","\n","あらかじめ与えられたデータは、赤い丸と、黒い四角になります。ここに新たなデー\n","タである灰色の三角が、丸と四角のどちらに分類されるかを予測します。k= 3 で予\n","測する場合は、赤い丸が2 つ、黒い四角が1 つなので、灰色の三角は赤い丸に分類さ\n","れます。一方、k= 5 の場合は、赤い丸が2 つ、黒い四角が3 つなので、灰色の三角\n","は黒い四角に分類されることになります。\n","\n","K 近傍法のメリットは、アルゴリズムが単純でわかりやすいことです。欠点は、次\n","元の数が大きいと類似度の距離が測りにくく、適用が難しくなることです。\n","\n","アイテムのレコメンデーションや、機械の故障などの異常値検出に使われます。\n","\n","<img src=\"https://github.com/t-date/DataScience/blob/master/fig/04_08_01.jpg?raw=true\" width=\"280px\">\n","\n","4-1で使用したOECDとGDPのデータを使用しk近傍法を実装してみましょう。"]},{"cell_type":"code","metadata":{"id":"hTiUJFOzOPuT","colab_type":"code","colab":{}},"source":["#データの取り込み\n","!wget -nc /content 'https://raw.githubusercontent.com/t-date/DataScience/master/data/oecd_bli_2015.csv'\n","!wget -nc /content 'https://raw.githubusercontent.com/t-date/DataScience/master/data/gdp_per_capita.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XhHAJPXtR7SM","colab_type":"code","colab":{}},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","# データをロードする\n","oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=\",\")\n","gdp_per_capita =pd.read_csv(\"gdp_per_capita.csv\",thousands=\",\",delimiter=\"\\t\", encoding=\"latin1\", na_values=\"n/a\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pz9wG1eER-HO","colab_type":"code","colab":{}},"source":["def prepare_country_stats(oecd_bli, gdp_per_capita):\n","    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n","    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n","    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n","    gdp_per_capita.set_index(\"Country\", inplace=True)\n","    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n","                                  left_index=True, right_index=True)\n","    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n","    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n","    keep_indices = list(set(range(36)) - set(remove_indices))\n","    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrjdC7cCSAjo","colab_type":"code","colab":{}},"source":["# データを準備する\n","country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n","X = np.c_[country_stats[\"GDP per capita\"]]\n","y = np.c_[country_stats[\"Life satisfaction\"]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_1_hNU3SCVc","colab_type":"code","colab":{}},"source":["# データを可視化する\n","country_stats.plot(kind=\"scatter\", x=\"GDP per capita\", y=\"Life satisfaction\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3mre79NSTnW","colab_type":"text"},"source":["k 近傍法モデルを実装してみます。"]},{"cell_type":"code","metadata":{"id":"xXLPtBQRSOo6","colab_type":"code","colab":{}},"source":["# k 近傍法モデルを選択する\n","from sklearn import neighbors as nb\n","model = nb.KNeighborsRegressor(n_neighbors=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6O8fPrAS5VI","colab_type":"code","colab":{}},"source":["# モデルを訓練する\n","model.fit(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOLMxAZ4S8Jv","colab_type":"code","colab":{}},"source":["# キプロスの例から予測を行う\n","X_new = [[22587]] # キプロスの1 人あたりGDP\n","print(model.predict(X_new)) # 出力[[ 5.96242338]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmeSMpZrTC6Q","colab_type":"text"},"source":["予測モデルを引いてみます。"]},{"cell_type":"code","metadata":{"id":"AHN3vq3oTDkV","colab_type":"code","colab":{}},"source":["country_stats.plot(kind=\"scatter\", x=\"GDP per capita\", y=\"Life satisfaction\")\n","plt.plot(X, model.predict(X))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cR3QS7ivTRf9","colab_type":"text"},"source":["## クラスタリング(教師なし学習)\n","教師なし学習で典型的な例は、与えられたデータをいくつかのクラスターに分類するクラスタリングです。\n","\n","教師なし学習のクラスタリングで広く用いられる手法としては、\n","K-Means 法（またはK-平均法）があります。K-Means法と、K-近傍法(K-NN法)とは別物だということに注意してください。"]},{"cell_type":"markdown","metadata":{"id":"o_tDgX4KTfzF","colab_type":"text"},"source":["###K-Means法\n","**データをk個のクラスタに分け、各クラスタの重心に一番近い点をそのクラスタに分類しなおすということを繰り返して、データをクラスタリングする手法です。**\n","\n","シンプルなアルゴリズムのため、広く用いられています。\n","\n","最適なkを見積もる方法としては、SSE（残差平方和）の減少量を見るエルボー法、クラスタ内のデータの凝集度を見るシルエット法が知られています。\n","\n","k-meansクラスタリングは、最も単純で最も広く用いられているクラスタリングアルゴリズムであり、このアルゴリズムは、データのある領域を代表するようなクラスタ重心を見つけようとします。\n","\n","このアルゴリズムは次の2つのステップを繰り返します。個々のデータポイントを最寄りのクラスタ重心に割り当てる。次に、個々のクラスタ重心をその点に割り当てられたデータポイントの平均に設定する。データポイントの割り当てが変化しなくなったら、アルゴリズムは終了します。\n","※下図はイメージで星印は各クラスタの重心\n","\n","<img src=\"https://github.com/t-date/DataScience/blob/master/fig/04_08_02.jpg?raw=true\" width=\"280px\">"]},{"cell_type":"markdown","metadata":{"id":"h5fZSFS_bE0w","colab_type":"text"},"source":["scikit-learnでは数多くのクラスタリングアルゴリズムが提供されていますが、理解しやすく最も単純なアルゴリズムはk 平均法（k-means）クラスタリングと呼ばれ、sklearn.cluster.KMeans で実装されています。標準的なモジュールを最初にインポートします。"]},{"cell_type":"code","metadata":{"id":"SW6RnRh1bTn-","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set() # プロットのスタイルを決める\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tq_sYdEbE_I","colab_type":"text"},"source":["k-Means法のアルゴリズムは、ラベル付けされていない多次元データセット内で与えられた数のクラスタを探します。これは、最適なクラスタリングはどのように見えるかという、次のような単純な概念を使用します。\n","\n","*   「クラスタの中心」は、クラスタに属するすべてのポイントの算術平均である。\n","*   各ポイントは、他のクラスタ中心よりも自分の属するクラスタの中心に近い。\n","\n","この2 つの前提が、k-Means法モデルの基礎となりますまずは単純なデータセットにk 平均法を適用した結果を見てみましょう。\n","\n","まず、4つの異なる集団からなる2 次元のデータセットを生成します。これが教師なし学習アルゴリズムであることを強調するために、ラベルをプロットには含めません。\n","\n"]},{"cell_type":"code","metadata":{"id":"2gx3Vl_rbsSA","colab_type":"code","colab":{}},"source":["from sklearn.datasets.samples_generator import make_blobs\n","X, y_true = make_blobs(n_samples=300, centers=4,\n","cluster_std=0.60, random_state=0)\n","plt.scatter(X[:, 0], X[:, 1], s=50);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uUpp3Pn-buuo","colab_type":"text"},"source":["見た目では、4 つのクラスタを識別するのは比較的簡単です。k 平均法アルゴリズムはこれを自動的に行います。scikit-learnは典型的な推定器APIを提供します。"]},{"cell_type":"code","metadata":{"id":"ox4VC617bxb5","colab_type":"code","colab":{}},"source":["from sklearn.cluster import KMeans\n","kmeans = KMeans(n_clusters=4)\n","kmeans.fit(X)\n","y_kmeans = kmeans.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgoMOEcNb1-5","colab_type":"text"},"source":["得られたラベルで色付けしたデータをプロットして、結果を可視化しましょう。また、k-Means法推定器によって決定されたクラスタの中心もプロットします。"]},{"cell_type":"code","metadata":{"id":"HrONPZo3cLqo","colab_type":"code","colab":{}},"source":["plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8coMBY9YcPTG","colab_type":"text"},"source":["k-Means法アルゴリズム（少なくともこのような単純なケースでは）は見た目でクラスタを識別する方法とほぼ同じ方法で各点をクラスタに割り当てます。しかし、このアルゴリズムがいかにしてクラスタを素早く見つけ出すのか疑問に思うかもしれません。結局、クラスタ割り当て可能な組み合わ\n","せの数はデータポイントの数に対して指数関数的に増加するため、網羅的な検索は非常にコストが\n","かかります。幸いなことに、網羅的な検索は必要ありません。代わりに、k-Means法の典型的なアプ\n","ローチは、期待値最大化法と呼ばれる直感的な反復アプローチを用います。"]},{"cell_type":"markdown","metadata":{"id":"RbiJxGB-dA4p","colab_type":"text"},"source":["###期待値最大化法k-Means法"]},{"cell_type":"markdown","metadata":{"id":"Vzm39H6xbFH5","colab_type":"text"},"source":["期待値最大化法（E-M：Expectation-maximization）は、データサイエンスのさまざまな状況で使\n","用される強力なアルゴリズムです。k 平均法は、このアルゴリズムの特に簡単でわかりやすい応用\n","の1つです。簡単に説明しましょう。期待値最大化法は以下の手順で構成されています。\n","1.   クラスタ中心を推測する\n","2.   収束するまで繰り返す\n","> a. Eステップ：各ポイントを最近傍のクラスタ中心に割り当てる   \n","> b. Mステップ：各ポイントの平均をクラスタ中心に設定する\n","\n","まず、各ポイントがどのクラスタに属するかの予想を更新する「Eステップ」または「期待値（Expectation）ステップ」を実行します。次に、ここでの場合はクラスタ中心を定義するフィットネス関数を最大化する「Mステップ」または「最大化（Maximization）ステップ」を実行します。この最大化は各クラスタに含まれるデータの単純な平均を取ることで行われます。\n","\n","このアルゴリズムに関する論文は膨大に存在しますが、以下のように要約することができます。\n","つまり典型的な状況下では、E ステップおよびMステップ繰り返しは、常にクラスタ特性のより\n","良い推定をもたらします。\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"noMUiG5Rdsm_","colab_type":"code","colab":{}},"source":["from sklearn.metrics import pairwise_distances_argmin\n","def find_clusters(X, n_clusters, rseed=2):\n","  # 1. ランダムにクラスタを選択\n","  rng = np.random.RandomState(rseed)\n","  i = rng.permutation(X.shape[0])[:n_clusters]\n","  centers = X[i]\n","  while True:\n","    # 2a. 最近傍のクラスタでラベル付けを行う\n","    labels = pairwise_distances_argmin(X, centers)\n","  \n","  \n","    # 2b. ポイントの平均からクラスタ中心決定する\n","    new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n","    \n","    # 2c. 収束チェック\n","    if np.all(centers == new_centers):\n","      break\n","    \n","    centers = new_centers\n","  \n","  return centers, labels\n","\n","centers, labels = find_clusters(X, 4)\n","plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_OHSQoGbFTR","colab_type":"text"},"source":["十分にテストされた多くの実装は舞台裏でさらに多くの作業を行いますが、ここで示した関数は期待値最大化法アプローチの要点を示しています。"]}]}